{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Training of inception V3 model on aerial data\n",
    "instead of training a model to classify images into species directly, we will train a model to first classify them into leaf type, then two models which classify broadleaf and pineneedle trees into their respective families. This should hopefully result in a more general and more accurate classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the aerial hierarchy dictionary and image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open(r\"..\\..\\data\\general_hierarchy.json\") as file:\n",
    "    general_hierarchy = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading image data\n",
    "train_images = np.load(r\"C:\\Users\\bench\\OneDrive\\Documents\\EMAT Year 3\\MDM3\\Phase C\\ratio_adjusted_aerial_dataset\\train_images.npy\")\n",
    "train_labels = np.load(r\"C:\\Users\\bench\\OneDrive\\Documents\\EMAT Year 3\\MDM3\\Phase C\\ratio_adjusted_aerial_dataset\\train_labels.npy\")\n",
    "val_images = np.load(r\"C:\\Users\\bench\\OneDrive\\Documents\\EMAT Year 3\\MDM3\\Phase C\\ratio_adjusted_aerial_dataset\\val_images.npy\")\n",
    "val_labels = np.load(r\"C:\\Users\\bench\\OneDrive\\Documents\\EMAT Year 3\\MDM3\\Phase C\\ratio_adjusted_aerial_dataset\\val_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the initial binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_classes = 2\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broadleaf': ['Quercus_rubra', 'Quercus_robur', 'Quercus_petraea', 'Fagus_sylvatica', 'Acer_pseudoplatanus', 'Fraxinus_excelsior', 'Tilia_spec.', 'Prunus_spec.', 'Alnus_spec.', 'Populus_spec.', 'Betula_spec.'], 'conifer': ['Picea_abies', 'Abies_alba', 'Pseudotsuga_menziesii', 'Pinus_sylvestris', 'Pinus_nigra', 'Pinus_strobus', 'Larix_decidua', 'Larix_kaempferi']}\n",
      "[1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#getting two lists of species for binary classification\n",
    "binary_species = {\"broadleaf\":[], \"conifer\":[]}\n",
    "for leaf_type, foliage in zip([\"broadleaf\",\"conifer\"], general_hierarchy):\n",
    "    for family in general_hierarchy[foliage]:\n",
    "        for species in general_hierarchy[foliage][family]:\n",
    "            binary_species[leaf_type].append(species)\n",
    "\n",
    "print(binary_species)\n",
    "\n",
    "# converting image labels to binary labels, 0 represents broadleaf and 1 represents conifer\n",
    "def getFoliageLabels(aerial_labels, binary_species):\n",
    "    foliage_labels = np.empty((aerial_labels.shape[0],))\n",
    "    foliage_labels.fill(np.nan)\n",
    "    broadleaf_index = np.isin(aerial_labels, binary_species[\"broadleaf\"])\n",
    "    conifer_index = np.isin(aerial_labels, binary_species[\"conifer\"])\n",
    "    \n",
    "    foliage_labels[broadleaf_index] = 0\n",
    "    foliage_labels[conifer_index] = 1\n",
    "    foliage_labels = foliage_labels.astype(\"uint8\")\n",
    "    \n",
    "    #check if any labels are not broadleaf or conifer\n",
    "    if np.count_nonzero(foliage_labels==np.nan) != 0:\n",
    "        raise ValueError(\"Some labels are not broadleaf or conifer\")\n",
    "    \n",
    "    return foliage_labels\n",
    "\n",
    "foliage_train_labels = getFoliageLabels(train_labels, binary_species)\n",
    "foliage_val_labels = getFoliageLabels(val_labels, binary_species)\n",
    "\n",
    "#displaying some random labels\n",
    "print(foliage_train_labels[np.random.randint(0, foliage_train_labels.shape[0], 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verifying data dimensions are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels:  (14094,)\n",
      "val labels:  (1762,)\n",
      "train images:  (14094, 304, 304, 4)\n",
      "val images:  (1762, 304, 304, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"train labels: \",foliage_train_labels.shape)\n",
    "print(\"val labels: \",foliage_val_labels.shape)\n",
    "print(\"train images: \",train_images.shape)\n",
    "print(\"val images: \",val_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bench\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in C:\\Users\\bench/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "from training_funcs import initialize_inception_model\n",
    "\n",
    "foliage_model, input_size = initialize_inception_model(num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_funcs import CustomDataset\n",
    "    \n",
    "#creating training and validation pytorch datasets\n",
    "training_dataset = CustomDataset(train_images, foliage_train_labels, input_size)\n",
    "val_dataset = CustomDataset(val_images, foliage_val_labels, input_size)\n",
    "batch_size = 32\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True),\n",
    "                    'val': torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters to train = 6\n",
      "parameter pre_layer.input_conv.weight: torch.Size([3, 4, 1, 1])\n",
      "parameter pre_layer.input_conv.bias: torch.Size([3])\n",
      "parameter inception_v3.AuxLogits.fc.weight: torch.Size([2, 768])\n",
      "parameter inception_v3.AuxLogits.fc.bias: torch.Size([2])\n",
      "parameter inception_v3.fc.weight: torch.Size([2, 2048])\n",
      "parameter inception_v3.fc.bias: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#checking the params we are training\n",
    "params_to_update = []\n",
    "names = []\n",
    "for name,param in foliage_model.named_parameters():\n",
    "  if param.requires_grad==True:\n",
    "    params_to_update.append(param)\n",
    "    names.append(name)\n",
    "\n",
    "print(\"number of parameters to train =\",len(params_to_update))\n",
    "for i, param in enumerate(params_to_update):\n",
    "  print(\"parameter {}:\".format(names[i]),param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending model to cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"sending model to\",device)\n",
    "# Send the model to GPU\n",
    "foliage_model = foliage_model.to(device)\n",
    "# Setup the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#initializing optimizer with hyperparameters determined from the random search\n",
    "optimizer = optim.AdamW(params_to_update, lr=0.0005, betas=(0.9, 0.995), weight_decay=0.015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5901 Acc: 0.7895\n",
      "val Loss: 0.3429 Acc: 0.8632\n",
      "epoch 0 complete\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.4872 Acc: 0.8427\n",
      "val Loss: 0.3154 Acc: 0.8655\n",
      "epoch 1 complete\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.4732 Acc: 0.8443\n",
      "val Loss: 0.3398 Acc: 0.8570\n",
      "epoch 2 complete\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.4769 Acc: 0.8472\n",
      "val Loss: 0.3007 Acc: 0.8700\n",
      "epoch 3 complete\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.4594 Acc: 0.8521\n",
      "val Loss: 0.2952 Acc: 0.8740\n",
      "epoch 4 complete\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.4568 Acc: 0.8529\n",
      "val Loss: 0.2858 Acc: 0.8854\n",
      "epoch 5 complete\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.4551 Acc: 0.8541\n",
      "val Loss: 0.2833 Acc: 0.8820\n",
      "epoch 6 complete\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.4599 Acc: 0.8527\n",
      "val Loss: 0.2728 Acc: 0.8871\n",
      "epoch 7 complete\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.4432 Acc: 0.8546\n",
      "val Loss: 0.2702 Acc: 0.8899\n",
      "epoch 8 complete\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.4456 Acc: 0.8546\n",
      "val Loss: 0.2775 Acc: 0.8820\n",
      "epoch 9 complete\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.4569 Acc: 0.8550\n",
      "val Loss: 0.2698 Acc: 0.8956\n",
      "epoch 10 complete\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.4567 Acc: 0.8505\n",
      "val Loss: 0.2840 Acc: 0.8848\n",
      "epoch 11 complete\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.4428 Acc: 0.8586\n",
      "val Loss: 0.2696 Acc: 0.8939\n",
      "epoch 12 complete\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.4484 Acc: 0.8559\n",
      "val Loss: 0.2720 Acc: 0.8882\n",
      "epoch 13 complete\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.4384 Acc: 0.8582\n",
      "val Loss: 0.2603 Acc: 0.8910\n",
      "epoch 14 complete\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.4312 Acc: 0.8624\n",
      "val Loss: 0.2690 Acc: 0.8825\n",
      "epoch 15 complete\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.4388 Acc: 0.8575\n",
      "val Loss: 0.2646 Acc: 0.8922\n",
      "epoch 16 complete\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.4454 Acc: 0.8571\n",
      "val Loss: 0.2676 Acc: 0.8865\n",
      "epoch 17 complete\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.4520 Acc: 0.8510\n",
      "val Loss: 0.2675 Acc: 0.8871\n",
      "epoch 18 complete\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.4435 Acc: 0.8552\n",
      "val Loss: 0.2713 Acc: 0.8888\n",
      "epoch 19 complete\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.4377 Acc: 0.8589\n",
      "val Loss: 0.2605 Acc: 0.8888\n",
      "epoch 20 complete\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.4364 Acc: 0.8594\n",
      "val Loss: 0.3046 Acc: 0.8689\n",
      "epoch 21 complete\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.4346 Acc: 0.8593\n",
      "val Loss: 0.2660 Acc: 0.8922\n",
      "epoch 22 complete\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.4338 Acc: 0.8636\n",
      "val Loss: 0.2717 Acc: 0.8859\n",
      "epoch 23 complete\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.4415 Acc: 0.8558\n",
      "val Loss: 0.2892 Acc: 0.8740\n",
      "epoch 24 complete\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.4317 Acc: 0.8621\n",
      "val Loss: 0.2634 Acc: 0.8899\n",
      "epoch 25 complete\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.4360 Acc: 0.8560\n",
      "val Loss: 0.2656 Acc: 0.8910\n",
      "epoch 26 complete\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.4387 Acc: 0.8591\n",
      "val Loss: 0.2682 Acc: 0.8944\n",
      "epoch 27 complete\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.4379 Acc: 0.8560\n",
      "val Loss: 0.2543 Acc: 0.8978\n",
      "epoch 28 complete\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.4376 Acc: 0.8616\n",
      "val Loss: 0.2772 Acc: 0.8916\n",
      "epoch 29 complete\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.4471 Acc: 0.8556\n",
      "val Loss: 0.2573 Acc: 0.8944\n",
      "epoch 30 complete\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.4322 Acc: 0.8610\n",
      "val Loss: 0.2623 Acc: 0.8984\n",
      "epoch 31 complete\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.4304 Acc: 0.8592\n",
      "val Loss: 0.2602 Acc: 0.8967\n",
      "epoch 32 complete\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.4324 Acc: 0.8592\n",
      "val Loss: 0.2517 Acc: 0.8967\n",
      "epoch 33 complete\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.4382 Acc: 0.8609\n",
      "val Loss: 0.2605 Acc: 0.8961\n",
      "epoch 34 complete\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.4353 Acc: 0.8601\n",
      "val Loss: 0.2678 Acc: 0.8927\n",
      "epoch 35 complete\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.4341 Acc: 0.8604\n",
      "val Loss: 0.2684 Acc: 0.8922\n",
      "epoch 36 complete\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.4314 Acc: 0.8608\n",
      "val Loss: 0.2666 Acc: 0.8899\n",
      "epoch 37 complete\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.4264 Acc: 0.8624\n",
      "val Loss: 0.2579 Acc: 0.8956\n",
      "epoch 38 complete\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.4346 Acc: 0.8576\n",
      "val Loss: 0.2654 Acc: 0.8916\n",
      "epoch 39 complete\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.4337 Acc: 0.8587\n",
      "val Loss: 0.2927 Acc: 0.8780\n",
      "epoch 40 complete\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.4287 Acc: 0.8609\n",
      "val Loss: 0.2598 Acc: 0.8950\n",
      "epoch 41 complete\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.4451 Acc: 0.8589\n",
      "val Loss: 0.2553 Acc: 0.8933\n",
      "epoch 42 complete\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.4335 Acc: 0.8567\n",
      "val Loss: 0.2472 Acc: 0.8961\n",
      "epoch 43 complete\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.4315 Acc: 0.8593\n",
      "val Loss: 0.2497 Acc: 0.9012\n",
      "epoch 44 complete\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.4292 Acc: 0.8616\n",
      "val Loss: 0.2588 Acc: 0.8899\n",
      "epoch 45 complete\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.4332 Acc: 0.8598\n",
      "val Loss: 0.2504 Acc: 0.8984\n",
      "epoch 46 complete\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.4324 Acc: 0.8610\n",
      "val Loss: 0.2466 Acc: 0.9035\n",
      "epoch 47 complete\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.4256 Acc: 0.8610\n",
      "val Loss: 0.2884 Acc: 0.8700\n",
      "epoch 48 complete\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.4443 Acc: 0.8553\n",
      "val Loss: 0.2566 Acc: 0.8973\n",
      "epoch 49 complete\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.4253 Acc: 0.8602\n",
      "val Loss: 0.2642 Acc: 0.8967\n",
      "epoch 50 complete\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.4331 Acc: 0.8600\n",
      "val Loss: 0.2583 Acc: 0.8933\n",
      "epoch 51 complete\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.4408 Acc: 0.8554\n",
      "val Loss: 0.2534 Acc: 0.8990\n",
      "epoch 52 complete\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.4336 Acc: 0.8578\n",
      "val Loss: 0.2458 Acc: 0.9024\n",
      "epoch 53 complete\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.4301 Acc: 0.8625\n",
      "val Loss: 0.2527 Acc: 0.8995\n",
      "epoch 54 complete\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.4284 Acc: 0.8583\n",
      "val Loss: 0.2674 Acc: 0.8854\n",
      "epoch 55 complete\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.4282 Acc: 0.8631\n",
      "val Loss: 0.2744 Acc: 0.8882\n",
      "epoch 56 complete\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.4280 Acc: 0.8634\n",
      "val Loss: 0.2788 Acc: 0.8820\n",
      "epoch 57 complete\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.4347 Acc: 0.8601\n",
      "val Loss: 0.2603 Acc: 0.8944\n",
      "epoch 58 complete\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.4301 Acc: 0.8605\n",
      "val Loss: 0.2515 Acc: 0.8995\n",
      "epoch 59 complete\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.4361 Acc: 0.8578\n",
      "val Loss: 0.2516 Acc: 0.8956\n",
      "epoch 60 complete\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.4296 Acc: 0.8594\n",
      "val Loss: 0.2501 Acc: 0.9001\n",
      "epoch 61 complete\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.4306 Acc: 0.8626\n",
      "val Loss: 0.2990 Acc: 0.8785\n",
      "epoch 62 complete\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.4267 Acc: 0.8602\n",
      "val Loss: 0.2471 Acc: 0.8978\n",
      "epoch 63 complete\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.4350 Acc: 0.8591\n",
      "val Loss: 0.2622 Acc: 0.8871\n",
      "epoch 64 complete\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.4188 Acc: 0.8652\n",
      "val Loss: 0.2483 Acc: 0.8978\n",
      "epoch 65 complete\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.4311 Acc: 0.8610\n",
      "val Loss: 0.2594 Acc: 0.8978\n",
      "epoch 66 complete\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.4344 Acc: 0.8603\n",
      "val Loss: 0.2779 Acc: 0.8814\n",
      "epoch 67 complete\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.4365 Acc: 0.8570\n",
      "val Loss: 0.2613 Acc: 0.8842\n",
      "epoch 68 complete\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.4241 Acc: 0.8661\n",
      "val Loss: 0.2436 Acc: 0.9058\n",
      "epoch 69 complete\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.4292 Acc: 0.8627\n",
      "val Loss: 0.2406 Acc: 0.9092\n",
      "epoch 70 complete\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.4278 Acc: 0.8597\n",
      "val Loss: 0.2657 Acc: 0.8888\n",
      "epoch 71 complete\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.4226 Acc: 0.8639\n",
      "val Loss: 0.2416 Acc: 0.8978\n",
      "epoch 72 complete\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.4197 Acc: 0.8652\n",
      "val Loss: 0.2487 Acc: 0.9064\n",
      "epoch 73 complete\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.4199 Acc: 0.8657\n",
      "val Loss: 0.2534 Acc: 0.8967\n",
      "epoch 74 complete\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.4308 Acc: 0.8614\n",
      "val Loss: 0.2490 Acc: 0.8973\n",
      "epoch 75 complete\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.4174 Acc: 0.8653\n",
      "val Loss: 0.2806 Acc: 0.8820\n",
      "epoch 76 complete\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.4297 Acc: 0.8605\n",
      "val Loss: 0.2602 Acc: 0.8893\n",
      "epoch 77 complete\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.4268 Acc: 0.8632\n",
      "val Loss: 0.2492 Acc: 0.8978\n",
      "epoch 78 complete\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.4226 Acc: 0.8644\n",
      "val Loss: 0.2642 Acc: 0.8927\n",
      "epoch 79 complete\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.4212 Acc: 0.8629\n",
      "val Loss: 0.2656 Acc: 0.8939\n",
      "epoch 80 complete\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.4321 Acc: 0.8599\n",
      "val Loss: 0.2522 Acc: 0.8922\n",
      "epoch 81 complete\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.8638\n",
      "val Loss: 0.2602 Acc: 0.8905\n",
      "epoch 82 complete\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.4268 Acc: 0.8632\n",
      "val Loss: 0.2485 Acc: 0.8973\n",
      "epoch 83 complete\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.4293 Acc: 0.8628\n",
      "val Loss: 0.2464 Acc: 0.8927\n",
      "epoch 84 complete\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.4287 Acc: 0.8629\n",
      "val Loss: 0.2572 Acc: 0.8961\n",
      "epoch 85 complete\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.4242 Acc: 0.8677\n",
      "val Loss: 0.2623 Acc: 0.8950\n",
      "epoch 86 complete\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.4192 Acc: 0.8675\n",
      "val Loss: 0.2517 Acc: 0.8984\n",
      "epoch 87 complete\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.4266 Acc: 0.8650\n",
      "val Loss: 0.2531 Acc: 0.8933\n",
      "epoch 88 complete\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.4143 Acc: 0.8687\n",
      "val Loss: 0.2593 Acc: 0.8984\n",
      "epoch 89 complete\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.4190 Acc: 0.8661\n",
      "val Loss: 0.2534 Acc: 0.8933\n",
      "epoch 90 complete\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.4210 Acc: 0.8648\n",
      "val Loss: 0.2457 Acc: 0.8939\n",
      "epoch 91 complete\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.4203 Acc: 0.8652\n",
      "val Loss: 0.2462 Acc: 0.8927\n",
      "epoch 92 complete\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.4165 Acc: 0.8672\n",
      "val Loss: 0.2496 Acc: 0.8995\n",
      "epoch 93 complete\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.4294 Acc: 0.8639\n",
      "val Loss: 0.2560 Acc: 0.8990\n",
      "epoch 94 complete\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.4165 Acc: 0.8687\n",
      "val Loss: 0.2425 Acc: 0.9024\n",
      "epoch 95 complete\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.4163 Acc: 0.8696\n",
      "val Loss: 0.2606 Acc: 0.9001\n",
      "epoch 96 complete\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.4238 Acc: 0.8624\n",
      "val Loss: 0.2425 Acc: 0.9001\n",
      "epoch 97 complete\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.4230 Acc: 0.8646\n",
      "val Loss: 0.2582 Acc: 0.8995\n",
      "epoch 98 complete\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.4233 Acc: 0.8645\n",
      "val Loss: 0.2375 Acc: 0.9018\n",
      "epoch 99 complete\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.4143 Acc: 0.8688\n",
      "val Loss: 0.2853 Acc: 0.8797\n",
      "epoch 100 complete\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.4217 Acc: 0.8639\n",
      "val Loss: 0.2512 Acc: 0.9001\n",
      "epoch 101 complete\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.4247 Acc: 0.8633\n",
      "val Loss: 0.2629 Acc: 0.8893\n",
      "epoch 102 complete\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.4108 Acc: 0.8688\n",
      "val Loss: 0.2776 Acc: 0.8797\n",
      "epoch 103 complete\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.4174 Acc: 0.8660\n",
      "val Loss: 0.2457 Acc: 0.9012\n",
      "epoch 104 complete\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.4194 Acc: 0.8636\n",
      "val Loss: 0.2505 Acc: 0.8950\n",
      "epoch 105 complete\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.4215 Acc: 0.8623\n",
      "val Loss: 0.2457 Acc: 0.9035\n",
      "epoch 106 complete\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.4275 Acc: 0.8635\n",
      "val Loss: 0.2472 Acc: 0.8978\n",
      "epoch 107 complete\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.4292 Acc: 0.8627\n",
      "val Loss: 0.2414 Acc: 0.9007\n",
      "epoch 108 complete\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.4167 Acc: 0.8665\n",
      "val Loss: 0.2479 Acc: 0.8995\n",
      "epoch 109 complete\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.4094 Acc: 0.8655\n",
      "val Loss: 0.2399 Acc: 0.9052\n",
      "epoch 110 complete\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.4162 Acc: 0.8660\n",
      "val Loss: 0.2891 Acc: 0.8814\n",
      "epoch 111 complete\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.4155 Acc: 0.8677\n",
      "val Loss: 0.2490 Acc: 0.8916\n",
      "epoch 112 complete\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.4222 Acc: 0.8662\n",
      "val Loss: 0.2530 Acc: 0.8956\n",
      "epoch 113 complete\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.4139 Acc: 0.8647\n",
      "val Loss: 0.2419 Acc: 0.9030\n",
      "epoch 114 complete\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.4157 Acc: 0.8677\n",
      "val Loss: 0.2519 Acc: 0.9012\n",
      "epoch 115 complete\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.4250 Acc: 0.8619\n",
      "val Loss: 0.2417 Acc: 0.9012\n",
      "epoch 116 complete\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.4145 Acc: 0.8670\n",
      "val Loss: 0.2464 Acc: 0.9069\n",
      "epoch 117 complete\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.4234 Acc: 0.8626\n",
      "val Loss: 0.2455 Acc: 0.9001\n",
      "epoch 118 complete\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.4300 Acc: 0.8631\n",
      "val Loss: 0.2614 Acc: 0.8933\n",
      "epoch 119 complete\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.4264 Acc: 0.8631\n",
      "val Loss: 0.2647 Acc: 0.8893\n",
      "epoch 120 complete\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.4183 Acc: 0.8653\n",
      "val Loss: 0.2463 Acc: 0.8984\n",
      "epoch 121 complete\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.4181 Acc: 0.8616\n",
      "val Loss: 0.2561 Acc: 0.8956\n",
      "epoch 122 complete\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.4220 Acc: 0.8638\n",
      "val Loss: 0.2554 Acc: 0.8973\n",
      "epoch 123 complete\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.4167 Acc: 0.8677\n",
      "val Loss: 0.2463 Acc: 0.8967\n",
      "epoch 124 complete\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.4210 Acc: 0.8652\n",
      "val Loss: 0.2533 Acc: 0.8967\n",
      "epoch 125 complete\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.4155 Acc: 0.8670\n",
      "val Loss: 0.2473 Acc: 0.8973\n",
      "epoch 126 complete\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.4151 Acc: 0.8674\n",
      "val Loss: 0.2555 Acc: 0.8944\n",
      "epoch 127 complete\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.4235 Acc: 0.8642\n",
      "val Loss: 0.2512 Acc: 0.9007\n",
      "epoch 128 complete\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.4228 Acc: 0.8654\n",
      "val Loss: 0.2522 Acc: 0.8967\n",
      "epoch 129 complete\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.4265 Acc: 0.8662\n",
      "val Loss: 0.2446 Acc: 0.9041\n",
      "epoch 130 complete\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.4149 Acc: 0.8675\n",
      "val Loss: 0.2477 Acc: 0.8967\n",
      "epoch 131 complete\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.4115 Acc: 0.8712\n",
      "val Loss: 0.2705 Acc: 0.8876\n",
      "epoch 132 complete\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.4145 Acc: 0.8671\n",
      "val Loss: 0.2428 Acc: 0.9024\n",
      "epoch 133 complete\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.4143 Acc: 0.8678\n",
      "val Loss: 0.2412 Acc: 0.9035\n",
      "epoch 134 complete\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.4249 Acc: 0.8620\n",
      "val Loss: 0.2423 Acc: 0.9041\n",
      "epoch 135 complete\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.4288 Acc: 0.8643\n",
      "val Loss: 0.2509 Acc: 0.8956\n",
      "epoch 136 complete\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.4203 Acc: 0.8669\n",
      "val Loss: 0.2519 Acc: 0.9041\n",
      "epoch 137 complete\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.4261 Acc: 0.8628\n",
      "val Loss: 0.2545 Acc: 0.8916\n",
      "epoch 138 complete\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.4231 Acc: 0.8646\n",
      "val Loss: 0.2601 Acc: 0.8961\n",
      "epoch 139 complete\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.4093 Acc: 0.8687\n",
      "val Loss: 0.2456 Acc: 0.9007\n",
      "epoch 140 complete\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.4203 Acc: 0.8628\n",
      "val Loss: 0.2660 Acc: 0.8893\n",
      "epoch 141 complete\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.4182 Acc: 0.8668\n",
      "val Loss: 0.2540 Acc: 0.8922\n",
      "epoch 142 complete\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.4211 Acc: 0.8660\n",
      "val Loss: 0.2514 Acc: 0.8950\n",
      "epoch 143 complete\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.4207 Acc: 0.8648\n",
      "val Loss: 0.2477 Acc: 0.9012\n",
      "epoch 144 complete\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.4178 Acc: 0.8680\n",
      "val Loss: 0.2474 Acc: 0.8995\n",
      "epoch 145 complete\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.4173 Acc: 0.8687\n",
      "val Loss: 0.2433 Acc: 0.9047\n",
      "epoch 146 complete\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.4220 Acc: 0.8652\n",
      "val Loss: 0.2480 Acc: 0.8978\n",
      "epoch 147 complete\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.4140 Acc: 0.8668\n",
      "val Loss: 0.2406 Acc: 0.9024\n",
      "epoch 148 complete\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.4264 Acc: 0.8625\n",
      "val Loss: 0.2476 Acc: 0.8944\n",
      "epoch 149 complete\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.4220 Acc: 0.8647\n",
      "val Loss: 0.2473 Acc: 0.9007\n",
      "epoch 150 complete\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.4250 Acc: 0.8660\n",
      "val Loss: 0.2450 Acc: 0.9058\n",
      "epoch 151 complete\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.4300 Acc: 0.8651\n",
      "val Loss: 0.2577 Acc: 0.9007\n",
      "epoch 152 complete\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.4209 Acc: 0.8661\n",
      "val Loss: 0.2459 Acc: 0.8990\n",
      "epoch 153 complete\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.4173 Acc: 0.8663\n",
      "val Loss: 0.2429 Acc: 0.9024\n",
      "epoch 154 complete\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.4235 Acc: 0.8650\n",
      "val Loss: 0.2492 Acc: 0.8984\n",
      "epoch 155 complete\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.4140 Acc: 0.8658\n",
      "val Loss: 0.2437 Acc: 0.9052\n",
      "epoch 156 complete\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.4147 Acc: 0.8672\n",
      "val Loss: 0.2426 Acc: 0.9041\n",
      "epoch 157 complete\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.4124 Acc: 0.8678\n",
      "val Loss: 0.2364 Acc: 0.9041\n",
      "epoch 158 complete\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.4152 Acc: 0.8678\n",
      "val Loss: 0.2581 Acc: 0.8973\n",
      "epoch 159 complete\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.4169 Acc: 0.8680\n",
      "val Loss: 0.2410 Acc: 0.8990\n",
      "epoch 160 complete\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.4088 Acc: 0.8698\n",
      "val Loss: 0.2376 Acc: 0.9030\n",
      "epoch 161 complete\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.4185 Acc: 0.8660\n",
      "val Loss: 0.2450 Acc: 0.9030\n",
      "epoch 162 complete\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.4193 Acc: 0.8679\n",
      "val Loss: 0.2416 Acc: 0.9103\n",
      "epoch 163 complete\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.4185 Acc: 0.8676\n",
      "val Loss: 0.2352 Acc: 0.9064\n",
      "epoch 164 complete\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.4202 Acc: 0.8673\n",
      "val Loss: 0.2369 Acc: 0.9126\n",
      "epoch 165 complete\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.4090 Acc: 0.8721\n",
      "val Loss: 0.2424 Acc: 0.9098\n",
      "epoch 166 complete\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.4196 Acc: 0.8668\n",
      "val Loss: 0.2346 Acc: 0.9086\n",
      "epoch 167 complete\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.4046 Acc: 0.8689\n",
      "val Loss: 0.2499 Acc: 0.9058\n",
      "epoch 168 complete\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.4086 Acc: 0.8718\n",
      "val Loss: 0.2320 Acc: 0.9086\n",
      "epoch 169 complete\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.4029 Acc: 0.8741\n",
      "val Loss: 0.2564 Acc: 0.8984\n",
      "epoch 170 complete\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.4086 Acc: 0.8693\n",
      "val Loss: 0.2321 Acc: 0.9052\n",
      "epoch 171 complete\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.4016 Acc: 0.8753\n",
      "val Loss: 0.2500 Acc: 0.9064\n",
      "epoch 172 complete\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.3987 Acc: 0.8766\n",
      "val Loss: 0.2422 Acc: 0.9098\n",
      "epoch 173 complete\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.4010 Acc: 0.8736\n",
      "val Loss: 0.2300 Acc: 0.9058\n",
      "epoch 174 complete\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.4035 Acc: 0.8736\n",
      "val Loss: 0.2465 Acc: 0.9007\n",
      "epoch 175 complete\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.3936 Acc: 0.8762\n",
      "val Loss: 0.2326 Acc: 0.9086\n",
      "epoch 176 complete\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.3949 Acc: 0.8756\n",
      "val Loss: 0.2244 Acc: 0.9092\n",
      "epoch 177 complete\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.3886 Acc: 0.8781\n",
      "val Loss: 0.2225 Acc: 0.9137\n",
      "epoch 178 complete\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.3880 Acc: 0.8770\n",
      "val Loss: 0.2453 Acc: 0.9018\n",
      "epoch 179 complete\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.3937 Acc: 0.8777\n",
      "val Loss: 0.2232 Acc: 0.9200\n",
      "epoch 180 complete\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.3850 Acc: 0.8814\n",
      "val Loss: 0.2679 Acc: 0.8905\n",
      "epoch 181 complete\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.3848 Acc: 0.8816\n",
      "val Loss: 0.2372 Acc: 0.8995\n",
      "epoch 182 complete\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.3907 Acc: 0.8780\n",
      "val Loss: 0.2174 Acc: 0.9188\n",
      "epoch 183 complete\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.3857 Acc: 0.8793\n",
      "val Loss: 0.2314 Acc: 0.9018\n",
      "epoch 184 complete\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.3875 Acc: 0.8763\n",
      "val Loss: 0.2187 Acc: 0.9160\n",
      "epoch 185 complete\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.3815 Acc: 0.8797\n",
      "val Loss: 0.2139 Acc: 0.9177\n",
      "epoch 186 complete\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.3901 Acc: 0.8773\n",
      "val Loss: 0.2341 Acc: 0.9041\n",
      "epoch 187 complete\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.3821 Acc: 0.8765\n",
      "val Loss: 0.2144 Acc: 0.9171\n",
      "epoch 188 complete\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.3847 Acc: 0.8807\n",
      "val Loss: 0.2209 Acc: 0.9092\n",
      "epoch 189 complete\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.3842 Acc: 0.8799\n",
      "val Loss: 0.2334 Acc: 0.9058\n",
      "epoch 190 complete\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.3727 Acc: 0.8816\n",
      "val Loss: 0.2191 Acc: 0.9137\n",
      "epoch 191 complete\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.3812 Acc: 0.8769\n",
      "val Loss: 0.2392 Acc: 0.9069\n",
      "epoch 192 complete\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.3766 Acc: 0.8818\n",
      "val Loss: 0.2408 Acc: 0.8961\n",
      "epoch 193 complete\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.3802 Acc: 0.8809\n",
      "val Loss: 0.2261 Acc: 0.9160\n",
      "epoch 194 complete\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.3797 Acc: 0.8817\n",
      "val Loss: 0.2360 Acc: 0.9075\n",
      "epoch 195 complete\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.3732 Acc: 0.8823\n",
      "val Loss: 0.2220 Acc: 0.9149\n",
      "epoch 196 complete\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.3820 Acc: 0.8800\n",
      "val Loss: 0.2262 Acc: 0.9126\n",
      "epoch 197 complete\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.3833 Acc: 0.8805\n",
      "val Loss: 0.2240 Acc: 0.9183\n",
      "epoch 198 complete\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.3763 Acc: 0.8820\n",
      "val Loss: 0.2265 Acc: 0.9069\n",
      "epoch 199 complete\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.8839\n",
      "val Loss: 0.2502 Acc: 0.8990\n",
      "epoch 200 complete\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.3747 Acc: 0.8826\n",
      "val Loss: 0.2235 Acc: 0.9109\n",
      "epoch 201 complete\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.3845 Acc: 0.8777\n",
      "val Loss: 0.2234 Acc: 0.9149\n",
      "epoch 202 complete\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.8813\n",
      "val Loss: 0.2233 Acc: 0.9109\n",
      "epoch 203 complete\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.3860 Acc: 0.8781\n",
      "val Loss: 0.2362 Acc: 0.9058\n",
      "epoch 204 complete\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.3856 Acc: 0.8768\n",
      "val Loss: 0.2252 Acc: 0.9115\n",
      "epoch 205 complete\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.3871 Acc: 0.8770\n",
      "val Loss: 0.2398 Acc: 0.9041\n",
      "epoch 206 complete\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.3813 Acc: 0.8839\n",
      "val Loss: 0.2306 Acc: 0.9115\n",
      "epoch 207 complete\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.3778 Acc: 0.8817\n",
      "val Loss: 0.2259 Acc: 0.9103\n",
      "epoch 208 complete\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.3797 Acc: 0.8790\n",
      "val Loss: 0.2265 Acc: 0.9171\n",
      "epoch 209 complete\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.3648 Acc: 0.8858\n",
      "val Loss: 0.2288 Acc: 0.9086\n",
      "epoch 210 complete\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.3693 Acc: 0.8827\n",
      "val Loss: 0.2254 Acc: 0.9103\n",
      "epoch 211 complete\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.3743 Acc: 0.8817\n",
      "val Loss: 0.2384 Acc: 0.9098\n",
      "epoch 212 complete\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.3792 Acc: 0.8825\n",
      "val Loss: 0.2252 Acc: 0.9092\n",
      "epoch 213 complete\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.3849 Acc: 0.8804\n",
      "val Loss: 0.2285 Acc: 0.9126\n",
      "epoch 214 complete\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.3737 Acc: 0.8782\n",
      "val Loss: 0.2235 Acc: 0.9132\n",
      "epoch 215 complete\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.3697 Acc: 0.8831\n",
      "val Loss: 0.2219 Acc: 0.9149\n",
      "epoch 216 complete\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.3776 Acc: 0.8845\n",
      "val Loss: 0.2310 Acc: 0.9137\n",
      "epoch 217 complete\n",
      "Epoch 218/299\n",
      "----------\n",
      "Training interrupted\n",
      "Training complete in 673m 23s\n",
      "Best val Acc: 0.919977\n"
     ]
    }
   ],
   "source": [
    "from training_funcs import train_model\n",
    "import os\n",
    "import torch\n",
    "#fixing random seed for consistent results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#setting the number of epochs\n",
    "num_epochs = 300\n",
    "# Train and evaluate\n",
    "foliage_model, convergence_dict = train_model(foliage_model,\n",
    "                                                dataloaders_dict,\n",
    "                                                criterion,\n",
    "                                                optimizer,\n",
    "                                                num_epochs=num_epochs,\n",
    "                                                is_inception=True,\n",
    "                                                tensorboard_writer=None,\n",
    "                                                early_stopping=False,\n",
    "                                                device=device)\n",
    "\n",
    "#saving the model\n",
    "filename = \"foliage_model.pth\"\n",
    "if not os.path.exists(filename) or filename == \"temp.pth\":\n",
    "    torch.save(foliage_model.state_dict(),'foliage_model.pth')\n",
    "else:\n",
    "    print(\"model file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the convergence dictionary\n",
    "with open(\"foliage_convergence.json\", \"w\") as file:\n",
    "    json.dump(convergence_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the conifer family classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conifer_dict = general_hierarchy[\"conifer\"]\n",
    "\n",
    "#getting images corresponding to conifers\n",
    "conifer_species = []\n",
    "for i, family in enumerate(conifer_dict):\n",
    "    for species in conifer_dict[family]:\n",
    "        print(species)\n",
    "        conifer_species.append(species)\n",
    "\n",
    "# converting species labels, to numeric family labels\n",
    "def getFamilyLabels(aerial_labels, species_list):\n",
    "    foliage_labels = np.empty((aerial_labels.shape[0],))\n",
    "    foliage_labels.fill(np.nan)\n",
    "    broadleaf_index = np.isin(aerial_labels, binary_species[\"broadleaf\"])\n",
    "    conifer_index = np.isin(aerial_labels, binary_species[\"conifer\"])\n",
    "    \n",
    "    foliage_labels[broadleaf_index] = 0\n",
    "    foliage_labels[conifer_index] = 1\n",
    "    foliage_labels = foliage_labels.astype(\"uint8\")\n",
    "    \n",
    "    #check if any labels are not broadleaf or conifer\n",
    "    if np.count_nonzero(foliage_labels==np.nan) != 0:\n",
    "        raise ValueError(\"Some labels are not broadleaf or conifer\")\n",
    "    \n",
    "    return foliage_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(family_list)\n",
    "print(np.unique(conifer_labels))\n",
    "print(conifer_labels.shape)\n",
    "print(conifer_images.shape)\n",
    "print(conifer_labels[0:1000])\n",
    "\n",
    "#verifying that there are no zeros contained in the final list of labels\n",
    "if np.count_nonzero(conifer_labels == 0) != 0:\n",
    "    print(np.where(conifer_labels == 0))\n",
    "    raise ValueError(\"unlabelled data present\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
