{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1ZVOeO0QbfMuLF0j62GqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adampotton/MDM3-Rep-3/blob/main/S2_MODELS/S2_CNN_model4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1PwhIeXHdQJ",
        "outputId": "84b52380-26ff-48fc-ea75-af39757b2e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import files, drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load csv of the amount of tree in each sample and choose classifciation mode"
      ],
      "metadata": {
        "id": "9AD_7kmg8xr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 = species level (19 classes)\n",
        "# 1 = genus level (14 classes)\n",
        "# 2 = stand type (9 classes)\n",
        "# 3 = foliage type (2 classes)\n",
        "classification = 3\n",
        "\n",
        "table = pd.read_csv('/content/drive/My Drive/S2 Data/labels.csv')\n",
        "mapping = pd.read_csv('/content/drive/My Drive/S2 Data/tree_mapping.csv')\n",
        "df = pd.DataFrame(mapping)\n",
        "\n",
        "def extract_numbers(filename):\n",
        "    return filename.split('_')[-3].replace('.tif', '')\n",
        "\n",
        "table['File Number'] = table['File Name'].apply(extract_numbers)\n"
      ],
      "metadata": {
        "id": "4yGHMykd8vNX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract only the samples with pretty much just one sample and data split"
      ],
      "metadata": {
        "id": "QFPvHWv9o87T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = np.load('/content/drive/My Drive/S2 Data All/numbers.npy')\n",
        "\n",
        "tol = 0.99\n",
        "\n",
        "table_numeric = table.apply(pd.to_numeric, errors='coerce')\n",
        "columns_to_check = table_numeric.columns.difference(['File Number'])\n",
        "filtered_table = table_numeric[(table_numeric[columns_to_check] >= tol).any(axis=1)]\n",
        "\n",
        "indexes = np.where(np.isin(numbers, filtered_table['File Number']))[0]\n",
        "\n",
        "arrays = np.load('/content/drive/My Drive/S2 Data All/arrays.npy')\n",
        "labels = np.load('/content/drive/My Drive/S2 Data All/labels.npy')\n",
        "\n",
        "arrays_reduced = arrays[indexes]\n",
        "labels_reduced = labels[indexes]\n",
        "\n",
        "train_images, rest_images, train_names, rest_names = train_test_split(\n",
        "    arrays_reduced, labels_reduced, test_size=0.2, random_state=2, stratify=labels_reduced\n",
        ")\n",
        "\n",
        "test_images, val_images, test_names, val_names = train_test_split(\n",
        "    rest_images, rest_names, test_size=0.5, random_state=2, stratify=rest_names\n",
        ")"
      ],
      "metadata": {
        "id": "OMSZXNAYnIts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert labels array to correct classes and a numbered labels array\n"
      ],
      "metadata": {
        "id": "xldyY39BKGDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def map_tree_name(name, classification):\n",
        "    name = name.replace('_', ' ')\n",
        "    row_number = df.index[df['Species'] == name][0]\n",
        "    if classification == 0:\n",
        "        return name\n",
        "    elif classification == 1:\n",
        "        return df.iloc[row_number, 1]\n",
        "    elif classification == 2:\n",
        "        return df.iloc[row_number, 2]\n",
        "    elif classification == 3:\n",
        "        return df.iloc[row_number, 3]\n",
        "\n",
        "def apply_mapping(names, classification):\n",
        "    return np.array([map_tree_name(name, classification) for name in names])\n",
        "\n",
        "train_names2 = apply_mapping(train_names, classification)\n",
        "test_names2 = apply_mapping(test_names, classification)\n",
        "val_names2 = apply_mapping(val_names, classification)\n",
        "\n",
        "tree_names, train_labels = np.unique(train_names2, return_inverse=True)\n",
        "tree_names, test_labels = np.unique(test_names2, return_inverse=True)\n",
        "tree_names, val_labels = np.unique(val_names2, return_inverse=True)\n",
        "\n",
        "train_images = train_images.astype(np.float32)\n",
        "test_images = test_images.astype(np.float32)\n",
        "val_images = val_images.astype(np.float32)\n",
        "\n",
        "\n",
        "mean = np.mean(np.concatenate([train_images, test_images, val_images], axis=0), axis=(0, 1, 2))\n",
        "std = np.std(np.concatenate([train_images, test_images, val_images], axis=0), axis=(0, 1, 2))\n",
        "\n",
        "train_images = (train_images - mean) / std\n",
        "test_images = (test_images - mean) / std\n",
        "val_images = (val_images - mean) / std\n"
      ],
      "metadata": {
        "id": "6MUeipg_HjRE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Data Loaders\n"
      ],
      "metadata": {
        "id": "XoafB2foKWYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "train_images_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_images_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "val_images_tensor = torch.tensor(val_images, dtype=torch.float32)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
        "val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "REQRFpMoIrVw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the Model, and other parameters"
      ],
      "metadata": {
        "id": "pYrjWrBiysYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_channels = 12\n",
        "num_classes = len(tree_names)\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(num_channels, 32, kernel_size=3, padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Dropout(0.2),\n",
        "\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Dropout(0.2),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 1 * 1, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, num_classes)\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.00005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2)"
      ],
      "metadata": {
        "id": "vDNVj-0mkHwr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train, test, and validate, plotting validation loss"
      ],
      "metadata": {
        "id": "LQvs2GxMyyIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(images.permute(0, 3, 1, 2))\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def validation(epoch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(images.permute(0, 3, 1, 2))\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predicted = torch.argmax(logits, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "        val_loss /= len(test_loader)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f'Validation accuracy after {epoch+1} epochs: {accuracy:.2f}%')\n",
        "    return val_loss\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(images.permute(0, 3, 1, 2))\n",
        "\n",
        "            predicted = torch.argmax(logits, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            accuracy = 100 * correct / total\n",
        "        print(f'Test accuracy {100 * correct / total:.2f}%')\n",
        "\n",
        "num_epochs = 100\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    train()\n",
        "    val_loss = validation(epoch)\n",
        "    val_losses.append(val_loss)\n",
        "test()\n",
        "\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "9t58_XuLmovi",
        "outputId": "dbc5f76a-c67b-4caf-aa5e-8f7467b7d8d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy after 2 epochs: 95.22%\n",
            "Validation accuracy after 4 epochs: 95.45%\n",
            "Validation accuracy after 6 epochs: 95.62%\n",
            "Validation accuracy after 8 epochs: 95.95%\n",
            "Validation accuracy after 10 epochs: 95.45%\n",
            "Validation accuracy after 12 epochs: 95.73%\n",
            "Validation accuracy after 14 epochs: 96.07%\n",
            "Validation accuracy after 16 epochs: 96.01%\n",
            "Validation accuracy after 18 epochs: 96.07%\n",
            "Validation accuracy after 20 epochs: 96.01%\n",
            "Validation accuracy after 22 epochs: 95.90%\n",
            "Validation accuracy after 24 epochs: 95.67%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-140d7974d861>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-140d7974d861>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}