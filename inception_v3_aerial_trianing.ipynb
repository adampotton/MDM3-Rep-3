{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adampotton/MDM3-Rep-3/blob/main/inception_v3_aerial_trianing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArnHp1gGfHfL"
      },
      "source": [
        "# Mounting Google Drive and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdLp-2Wbdo6U",
        "outputId": "12bed47c-4dbc-4ed9-ddae-11a8effbc55a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPg69w9fcGf-"
      },
      "source": [
        "# Loading Numpy Data Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM3cA_OMfD8X",
        "outputId": "65d32fd9-7aaa-4b41-db63-e458fb80ae0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n",
            "['aerial_pseudotsuga_menziesii', 'aerial_picea_abies', 'aerial_betula_spec', 'aerial_abies_alba', 'aerial_pinus_nigra', 'aerial_larix_decidua', 'aerial_larix_kaempferi', 'aerial_pinus_strobus', 'aerial_pinus_sylvestris', 'aerial_fraxinus_excelsior', 'aerial_fagus_sylvatica', 'aerial_acer_pseudoplatanus', 'aerial_quercus_robur', 'aerial_quercus_petraea', 'aerial_quercus_rubra', 'aerial_tilia_spec', 'aerial_populus_spec', 'aerial_prunus_spec', 'aerial_alnus_spec', 'train_images.npy', 'train_labels.npy', 'test_images.npy', 'test_labels.npy', 'val_images.npy', 'val_labels.npy']\n"
          ]
        }
      ],
      "source": [
        "#if you get an error the first time just run it again\n",
        "os.chdir('../../../')\n",
        "!pwd\n",
        "os.chdir('content')\n",
        "os.chdir(r'drive/MyDrive/Aerial Data')\n",
        "print(os.listdir())\n",
        "\n",
        "train_labels = np.load('train_labels.npy')\n",
        "train_images = np.load('train_images.npy')\n",
        "test_labels = np.load('test_labels.npy')\n",
        "test_images = np.load('test_images.npy')\n",
        "val_labels = np.load('val_labels.npy')\n",
        "val_images = np.load('val_images.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# If Not using colab the following can be used to import the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7lEmQDmf06beJUD3NaQk82125NGqYpm\n",
            "To: c:\\Users\\bench\\OneDrive\\Documents\\GitHub\\MDM3-Rep-3\\data\\aerial_data\\val_labels.npy\n",
            "100%|██████████| 38.9k/38.9k [00:00<?, ?B/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-54Z1sN8uYgNG66MwWumRTlYMJmX7wWw\n",
            "From (redirected): https://drive.google.com/uc?id=1-54Z1sN8uYgNG66MwWumRTlYMJmX7wWw&confirm=t&uuid=510fa68d-caf5-408a-9a59-45cf796a8dc1\n",
            "To: c:\\Users\\bench\\OneDrive\\Documents\\GitHub\\MDM3-Rep-3\\data\\aerial_data\\val_images.npy\n",
            "100%|██████████| 171M/171M [00:14<00:00, 11.6MB/s] \n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1aJkI-7d5KCAAYCoXkrEcHVXvSsXc_-8n\n",
            "From (redirected): https://drive.google.com/uc?id=1aJkI-7d5KCAAYCoXkrEcHVXvSsXc_-8n&confirm=t&uuid=337c523f-4708-43a3-8990-a3ffb1a8c052\n",
            "To: c:\\Users\\bench\\OneDrive\\Documents\\GitHub\\MDM3-Rep-3\\data\\aerial_data\\train_labels.npy\n",
            "100%|██████████| 1.36G/1.36G [01:57<00:00, 11.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vKs-th9eHU_oMXTasVm-FD9Dximer82Q\n",
            "To: c:\\Users\\bench\\OneDrive\\Documents\\GitHub\\MDM3-Rep-3\\data\\aerial_data\\train_images.npy\n",
            "100%|██████████| 310k/310k [00:00<00:00, 4.35MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "\n",
        "gdown.download(r'https://drive.google.com/file/d/1-7lEmQDmf06beJUD3NaQk82125NGqYpm/view?usp=drive_link', r'data/aerial_data/val_labels.npy',fuzzy=True)\n",
        "gdown.download(r'https://drive.google.com/file/d/1-54Z1sN8uYgNG66MwWumRTlYMJmX7wWw/view?usp=drive_link', r'data/aerial_data/val_images.npy',fuzzy=True)\n",
        "gdown.download(r'https://drive.google.com/file/d/1vKs-th9eHU_oMXTasVm-FD9Dximer82Q/view?usp=drive_link', r'data/aerial_data/train_labels.npy',fuzzy=True)\n",
        "gdown.download(r'https://drive.google.com/file/d/1aJkI-7d5KCAAYCoXkrEcHVXvSsXc_-8n/view?usp=drive_link', r'data/aerial_data/train_images.npy',fuzzy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "folder = 'data/aerial_data'\n",
        "train_images = np.load(os.path.join(folder, 'train_images.npy'))\n",
        "train_labels = np.load(os.path.join(folder, 'train_labels.npy'))\n",
        "val_images = np.load(os.path.join(folder, 'val_images.npy'))\n",
        "val_labels = np.load(os.path.join(folder, 'val_labels.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zFEOXefkLVV",
        "outputId": "0d22abcb-ead4-4531-a9a7-7379b33404c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data:\n",
            "images shape:\n",
            "(3688, 304, 304, 4)\n",
            "labels shape:\n",
            "(3688,)\n",
            "val data:\n",
            "images shape:\n",
            "(462, 304, 304, 4)\n",
            "labels shape:\n",
            "(462,)\n"
          ]
        }
      ],
      "source": [
        "#inspecting data\n",
        "print(\"train data:\")\n",
        "print(\"images shape:\")\n",
        "print(train_images.shape)\n",
        "print(\"labels shape:\")\n",
        "print(train_labels.shape)\n",
        "\n",
        "#print(\"test data:\")\n",
        "#print(\"images shape:\")\n",
        "#print(test_images.shape)\n",
        "#print(\"labels shape:\")\n",
        "#print(test_labels.shape)\n",
        "\n",
        "print(\"val data:\")\n",
        "print(\"images shape:\")\n",
        "print(val_images.shape)\n",
        "print(\"labels shape:\")\n",
        "print(val_labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZptPJtcRl5"
      },
      "source": [
        "# Performing Feature Extraction on Inception-V3, which is currently trained on image net\n",
        "Adapted from: <https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/finetuning_torchvision_models_tutorial.ipynb#scrollTo=NiFT4EYmprqG>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HzgA5ZHDUKo"
      },
      "source": [
        "# Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYkVMsSdcXlF",
        "outputId": "50a4c307-1d55-46d0-a133-67aa651d0f24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\bench\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# Number of tree species categories\n",
        "num_classes = 19\n",
        "\n",
        "# Batch size for training (should be a power of 2 if training on the GPU and is limited by memory)\n",
        "batch_size = 4\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 15\n",
        "\n",
        "#True if we are just training the output layer of the network\n",
        "feature_extract = True\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00lUag6fDajn"
      },
      "source": [
        "# Function For training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aK47xAU8eqqM"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=True):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            #deep copying the model if its performance has improved\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print(\"epoch {} complete\".format(epoch))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights and return best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XIgz_TRkoyI"
      },
      "source": [
        "# Ensuring that model parameters are not used during training since we are only interested in the final layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P_1vlVVeknVh"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnqaEq2ibRf"
      },
      "source": [
        "# Defining function to reshape the model to account for the number of classes\n",
        "By default Inception-v3 has 1000 classes as it is trained on imageNet, whilst our dataset has 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pelFjz0lprqH"
      },
      "outputs": [],
      "source": [
        "def initialize_inception_model(num_classes, feature_extract, use_pretrained=True, model=None):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    \"\"\" Inception v3\n",
        "    Be careful, expects (299,299) sized images and has auxiliary output\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "      model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "    else:\n",
        "      model_ft = model\n",
        "    #\"switching off\" training on the majority of the model weights if feature_extract\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    # Handle the auxilary net\n",
        "    num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "    model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    # Handle the primary net\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "    input_size = 299\n",
        "\n",
        "    return model_ft, input_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvCE732PQME9"
      },
      "source": [
        "# Initializing Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdqIslSxQF09",
        "outputId": "578c2b85-4a50-49de-b584-a22c3bfe9a98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\bench\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\bench\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\bench/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:09<00:00, 11.5MB/s] \n"
          ]
        }
      ],
      "source": [
        "#initializing model\n",
        "#Initialize the model for this run\n",
        "model_ft, input_size = initialize_inception_model(num_classes, feature_extract, use_pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc8biARA66oL"
      },
      "source": [
        "# Formatting Loaded Data and creating data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iBUGQu37VXm",
        "outputId": "52176e3f-8f40-4acd-9999-3c833de0f5a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        }
      ],
      "source": [
        "# normalization for training and validation\n",
        "# hardcoded normalization values for the model\n",
        "# right now validation and training transform is the same and no augmentation\n",
        "image_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "#creating a custom pytorch dataset for the numpy arrays we loaded earlier\n",
        "#with applied transforms\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = torch.from_numpy(images)\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx,:,:,:]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "#creating training and validation pytorch datasets\n",
        "training_dataset = CustomDataset(train_images[:,:,:,1:], train_labels, image_transform)\n",
        "val_dataset = CustomDataset(val_images[:,:,:,1:], val_labels, image_transform)\n",
        "\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True),\n",
        "                    'val': torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIbdexT1A2l8"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxVIfck-A54U",
        "outputId": "e37efeb0-d405-4b4e-d3fe-cff967a9b7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sending model to cpu\n",
            "\t AuxLogits.fc.weight\n",
            "\t AuxLogits.fc.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n",
            "number of parameters to train = 4\n",
            "params to train =\n"
          ]
        }
      ],
      "source": [
        "## Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"sending model to\",device)\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Creating the Optimizer\n",
        "if feature_extract:\n",
        "  params_to_update = []\n",
        "  for name,param in model_ft.named_parameters():\n",
        "    if param.requires_grad==True:\n",
        "      params_to_update.append(param)\n",
        "      print(\"\\t\",name)\n",
        "else:\n",
        "  params_to_update = model_ft.parameters()\n",
        "\n",
        "print(\"number of parameters to train =\",len(params_to_update))\n",
        "print(\"params to train =\")\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcrgqxB0tf9E",
        "outputId": "b811be3e-cb65-4996-ec9c-6a62127fe24b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_ft, hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdataloaders_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                             \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mis_inception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_ft\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[13], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[0;32m     21\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Iterate over data.\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloaders[phase]:\n\u001b[0;32m     25\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[20], line 28\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 28\u001b[0m   image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\functional.py:141\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    139\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
          ]
        }
      ],
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft,\n",
        "                             dataloaders_dict,\n",
        "                             criterion,\n",
        "                             optimizer_ft,\n",
        "                             num_epochs=num_epochs,\n",
        "                             is_inception=True)\n",
        "\n",
        "torch.save(model_ft.state_dict(),'trained_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5VQeaagqhbr"
      },
      "source": [
        "#Testing Eval on the inception model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke-50XtvqmEF",
        "outputId": "356b105e-2ff7-4ea8-96ef-75dbef6c36aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 1.5696e-01,  1.1051e-01, -4.4363e-01, -6.1476e-02, -2.0093e-01,\n",
            "         1.6820e-01,  5.8117e-01,  6.0333e-02, -7.7650e-02, -1.1192e+00,\n",
            "        -2.1800e-01, -5.2624e-01, -1.1816e-01,  6.6276e-02,  8.4988e-01,\n",
            "        -5.3260e-02, -4.7113e-01, -2.5986e-03,  2.7504e-01,  1.7060e-01,\n",
            "         4.4231e-01, -3.8615e-01,  1.3482e-01, -5.2201e-01, -3.3595e-04,\n",
            "        -2.3492e-01, -2.3264e-01, -7.1890e-03,  1.6899e-02, -1.2724e-01,\n",
            "         2.0508e-01, -6.9896e-03,  1.3074e+00, -2.3181e-01,  9.8966e-01,\n",
            "        -7.7783e-01,  4.8900e-01, -4.2244e-01, -2.6523e-01, -1.5005e-01,\n",
            "         3.5147e-01, -1.7462e-01,  1.0251e-01,  4.2223e-01, -1.4342e-01,\n",
            "         1.3578e-02, -4.6477e-01,  3.5966e-01,  1.5194e-01,  2.0217e-01,\n",
            "        -1.3763e-01,  1.4986e-01,  2.8415e-01, -3.7086e-01,  9.7688e-01,\n",
            "         7.9543e-01, -7.4032e-01, -7.8853e-02, -9.6364e-02,  1.2193e-01,\n",
            "        -1.2712e-01,  3.6825e-01, -4.5354e-01, -7.4801e-01, -7.9353e-02,\n",
            "         2.1689e-01,  2.7018e-01,  1.0089e+00,  4.0730e-01, -1.8570e-01,\n",
            "        -9.2471e-02, -1.2458e-01, -1.0435e+00, -4.0775e-01,  1.8338e-01,\n",
            "        -2.7525e-01, -2.9294e-01,  1.6330e-02,  2.8760e-02,  2.0185e-01,\n",
            "        -4.8364e-01, -7.0668e-02,  1.6896e-01, -4.6968e-02,  8.4163e-01,\n",
            "        -3.6728e-01, -3.0113e-01, -3.7112e-01,  2.6911e-01,  3.7275e-01,\n",
            "         1.8055e-01,  3.1311e-01,  4.5380e-01, -1.3350e-01, -2.9444e-01,\n",
            "        -2.7947e-01,  8.4207e-01, -2.3105e-01, -3.1553e-01, -4.3883e-02,\n",
            "         1.5329e-01,  3.1552e-01,  1.8616e-01, -9.6860e-02,  4.7281e-01,\n",
            "        -8.9032e-01, -7.2464e-01,  2.3588e-01,  6.6725e-01,  1.3802e-01,\n",
            "        -2.4249e-01, -2.3606e-02, -5.4461e-02,  4.9207e-01,  4.9701e-02,\n",
            "        -5.8998e-01, -7.7230e-01,  1.3981e-01, -8.2220e-01,  3.9934e-02,\n",
            "         1.7365e-01, -1.0092e+00, -6.9965e-03, -2.6625e-01, -2.8959e-01,\n",
            "        -6.6461e-02, -6.4817e-01,  9.3626e-01,  5.5003e-01,  5.2842e-01,\n",
            "         1.0856e-01,  1.8085e-01,  6.0892e-01, -6.0835e-01,  3.3635e-01,\n",
            "         1.8864e-01,  5.8985e-01, -4.0940e-01, -7.3610e-01, -6.2800e-01,\n",
            "        -2.7593e-01,  2.6514e-01, -7.5665e-01,  2.5088e-01,  9.6007e-01,\n",
            "        -9.4859e-02,  3.0636e-01, -2.7370e-02,  3.5943e-02,  1.7601e-01,\n",
            "        -2.8860e-01,  1.6987e-01,  2.4232e+00, -1.2510e-01,  9.1045e-01,\n",
            "        -8.8260e-01, -5.1855e-02,  2.1003e+00, -6.6269e-01, -2.4619e-01,\n",
            "        -8.5638e-01, -1.3153e+00, -1.1042e+00, -2.4547e-01, -1.3737e-01,\n",
            "        -1.1962e-01, -2.0865e-01, -3.6327e-01, -9.5908e-02,  7.0066e-01,\n",
            "        -2.2363e-01, -1.8417e-01, -8.5439e-01, -1.8438e-01,  1.3066e+00,\n",
            "        -4.8575e-01, -5.6697e-01, -1.9955e-01, -1.6855e-01, -1.4573e-01,\n",
            "        -8.5888e-01, -3.4339e-01, -2.9275e-01, -4.5627e-01, -5.1301e-01,\n",
            "         4.3903e-01,  8.0301e-01, -4.9003e-01, -6.9808e-01, -8.5991e-01,\n",
            "        -4.1795e-01, -4.4660e-01,  2.0680e-01, -1.2265e+00, -4.5363e-01,\n",
            "        -5.2789e-01, -1.1172e+00,  2.6781e-01, -4.5858e-01,  2.8015e-01,\n",
            "         6.0328e-01, -8.8579e-01, -8.8179e-01,  1.4860e+00, -4.7116e-01,\n",
            "        -1.1044e+00, -5.9475e-01,  6.3241e-01, -5.9048e-01, -7.0500e-01,\n",
            "        -3.8798e-01, -6.3180e-01, -2.1158e-01, -1.6535e-01,  6.3881e-01,\n",
            "         2.8981e-02,  4.5989e-01,  8.4380e-02, -5.4091e-02, -1.9021e+00,\n",
            "        -1.1602e-01, -2.3897e-01,  2.5432e+00,  1.7504e+00,  1.3806e+00,\n",
            "        -5.3402e-01,  3.4183e-01, -7.0692e-01,  3.1853e-01, -9.9137e-02,\n",
            "         1.1112e+00,  1.2860e+00,  1.0465e+00, -7.3275e-02, -2.3343e-01,\n",
            "         6.5052e-02,  5.4067e-01, -4.5558e-01, -8.5782e-02, -4.2102e-01,\n",
            "         7.2590e-01,  4.5586e-01, -8.7706e-02, -8.4019e-01, -3.2783e-01,\n",
            "        -5.2903e-01, -4.2849e-01,  4.9993e-01,  3.2688e+00,  2.3975e+00,\n",
            "         1.5311e+00, -2.0808e-01, -1.8428e-01, -2.8011e-01, -1.7842e-01,\n",
            "         2.8461e-01,  5.3146e-01,  2.8738e+00,  8.8523e+00,  4.2019e+00,\n",
            "         2.5316e+00,  3.9927e+00, -3.1314e-01, -6.2400e-01, -2.6323e-01,\n",
            "        -2.3428e-01, -3.3132e-01,  1.7870e-01, -4.0714e-01,  8.2185e-01,\n",
            "         4.6482e+00, -8.9971e-01, -3.8765e-01,  6.0829e-01,  3.4514e-01,\n",
            "        -8.4613e-01, -7.5298e-02,  5.9165e-01, -2.2570e-01,  4.7909e+00,\n",
            "         2.8114e-01, -3.3872e-01, -7.5726e-01,  1.2868e+00, -6.3885e-01,\n",
            "        -7.2032e-01, -1.0338e+00,  2.1799e-01, -5.7310e-02,  1.2854e-01,\n",
            "        -4.6508e-01,  3.5067e-02, -3.6578e-01, -4.7804e-02, -9.9203e-01,\n",
            "        -1.3356e+00,  6.5033e-01, -2.7167e-01, -6.8011e-01, -5.7143e-01,\n",
            "        -5.3475e-01, -3.8411e-01, -6.4809e-02, -2.8354e-01, -1.7806e-01,\n",
            "        -2.8930e-02,  2.4240e-01, -1.7973e-01,  1.8139e-01, -1.5519e-01,\n",
            "        -2.3687e-01,  1.5365e-01, -1.1674e-01, -2.9110e-01,  2.4691e-01,\n",
            "         2.1827e-01, -5.1025e-01,  2.5870e-01, -1.6245e-01, -2.7104e-01,\n",
            "         4.1199e-01, -3.9074e-01, -6.5319e-01,  2.0439e-01, -1.2711e+00,\n",
            "        -4.3313e-01,  1.5427e-01,  3.4154e-01,  8.7113e-01, -4.1166e-02,\n",
            "        -1.8232e-01,  3.9799e-01,  9.8626e-01, -4.9940e-01, -2.1770e-02,\n",
            "        -5.8458e-01, -9.3022e-01, -5.0841e-01,  1.4682e-02, -2.6315e-01,\n",
            "         1.1864e-02, -1.7344e-01, -2.2880e-01, -2.0762e-01, -2.8182e-01,\n",
            "         3.0906e-02,  5.7570e-01,  3.2597e-01, -1.9836e-01, -8.1337e-01,\n",
            "        -5.2461e-01, -1.6578e-01, -4.2071e-01, -1.4268e-01, -2.6060e-01,\n",
            "         1.3963e-01, -8.4469e-02, -2.1359e-01,  7.6789e-01,  5.5213e-01,\n",
            "        -1.0924e+00,  6.3716e-01, -7.3622e-01,  4.3754e-01, -7.7834e-01,\n",
            "        -7.9443e-01, -1.4501e+00, -7.9229e-01, -7.8784e-01, -8.8925e-01,\n",
            "         2.0367e-01, -4.4672e-01, -9.1861e-01,  6.8160e-02, -2.8454e-01,\n",
            "        -3.7750e-01, -9.7066e-01,  1.0829e-01, -4.9947e-01, -1.3005e+00,\n",
            "        -7.6300e-01, -6.5802e-01, -7.3246e-01, -1.0341e+00, -1.2842e+00,\n",
            "        -3.6779e-01, -9.8348e-01, -9.7598e-02, -4.1650e-01, -6.8749e-02,\n",
            "        -3.5764e-01, -3.6454e-01,  2.1046e-01,  5.5319e-01, -2.1323e-01,\n",
            "        -2.6107e-01,  4.5051e-01, -7.4765e-02,  5.1910e-01,  1.7004e-01,\n",
            "        -3.2710e-02, -1.0269e+00,  5.5582e-01,  1.5760e-01,  9.7134e-02,\n",
            "         1.8764e-01, -3.2566e-01, -3.2677e-01,  2.5147e-01, -2.8530e-01,\n",
            "        -5.9853e-01, -1.0255e+00, -5.6512e-01, -3.8384e-01, -4.1670e-01,\n",
            "        -5.8828e-01, -6.2122e-01,  1.9794e-01, -4.6862e-01,  1.3163e-01,\n",
            "        -2.0324e-01, -1.0149e+00, -2.3162e-01,  1.7283e-01, -4.8196e-01,\n",
            "         2.5326e-01,  1.6461e-01,  6.2810e-02, -2.2099e-01,  2.0254e-01,\n",
            "        -1.6539e-01, -3.9706e-01,  2.4437e-01,  1.6360e-01, -7.2684e-01,\n",
            "        -1.0392e-02, -1.9011e-02, -3.1348e-01, -5.3056e-01,  8.2012e-02,\n",
            "         5.7136e-01, -3.0761e-01,  3.8100e-01, -6.4842e-01,  8.9464e-02,\n",
            "         4.7818e-01, -7.8970e-01, -3.4117e-01,  4.7725e-01,  1.5669e-01,\n",
            "        -1.8339e-01, -1.3243e-01,  2.3159e-01, -5.0399e-01, -2.2604e-01,\n",
            "         7.9325e-02,  5.2133e-01,  1.5784e-01, -8.6525e-02,  3.0305e-01,\n",
            "         9.7811e-02, -1.2771e+00,  7.0310e-01,  1.4597e-01,  3.2879e-01,\n",
            "        -3.6155e-01, -6.2039e-01, -8.8755e-01,  4.2935e-01,  1.1274e-01,\n",
            "         3.2891e-01,  8.4746e-02,  6.9847e-02,  8.1302e-01, -8.0782e-01,\n",
            "        -6.8245e-01,  1.3358e+00, -2.6256e-01,  8.9051e-01,  1.6758e-01,\n",
            "        -1.2170e-01,  4.2807e-01, -1.0054e+00,  3.5863e-01,  3.6241e-01,\n",
            "        -8.2987e-01,  2.9833e-01,  3.9783e-02,  3.5029e-01, -1.9784e-04,\n",
            "         1.4195e-02,  4.7898e-01, -5.8930e-01, -2.8985e-01, -1.0249e-01,\n",
            "         7.4155e-01,  2.6986e-01, -4.4872e-01, -1.5987e-01,  4.8277e-01,\n",
            "        -6.3190e-01, -2.1434e-01,  5.0953e-01, -6.9076e-01, -7.4333e-01,\n",
            "         2.0042e-01,  7.0911e-02,  1.0658e-02,  1.4495e-01, -6.7097e-01,\n",
            "         2.4766e-01,  7.7923e-01,  5.2101e-02, -3.9851e-01,  1.6331e-01,\n",
            "         1.0057e-01, -9.3775e-01, -5.3325e-01, -7.2511e-01, -1.2609e+00,\n",
            "        -2.4719e-01,  7.4883e-01,  1.0540e+00, -6.8609e-01, -1.6087e-01,\n",
            "        -4.3294e-01, -5.9875e-01, -7.0205e-01, -3.7986e-01, -5.7893e-01,\n",
            "         3.0747e-01, -2.1242e-02,  3.5246e-01, -2.5119e-01, -9.3544e-04,\n",
            "        -3.0555e-01, -1.0011e+00,  6.5736e-01, -1.9256e-02,  5.2677e-01,\n",
            "        -1.0669e-01, -3.7792e-01, -5.3913e-01,  1.4208e-01,  4.4693e-01,\n",
            "        -3.1716e-01,  3.0779e-01, -9.3855e-02, -6.7916e-01,  2.7398e-01,\n",
            "        -1.5625e-01,  2.8923e-01, -1.1230e-01, -1.1010e+00, -7.9937e-02,\n",
            "        -4.0612e-01,  2.0850e-02,  3.4710e-01,  4.3526e-02,  4.1307e-01,\n",
            "        -2.4231e-01, -9.4655e-01,  5.9509e-01,  4.2503e-01,  2.0579e-01,\n",
            "        -2.1274e-01,  3.5350e-02,  2.6313e-01, -2.9305e-01, -4.8487e-01,\n",
            "        -1.3051e-01, -6.9740e-01,  2.1814e-04,  7.6510e-01, -1.5280e-01,\n",
            "        -3.3351e-01,  3.0998e-01, -1.2360e-01,  3.9550e-01, -2.5940e-01,\n",
            "         6.4828e-02,  2.7397e-01, -1.5634e-01, -1.5505e-03,  6.4023e-01,\n",
            "         4.5707e-01, -7.3576e-01,  3.0117e-01, -4.9941e-01, -7.1723e-02,\n",
            "         4.7881e-01,  3.9969e-02,  6.5618e-01,  4.1189e-01,  4.7485e-01,\n",
            "        -6.1536e-01,  9.2373e-01,  1.2427e-01, -6.0887e-01, -3.6679e-01,\n",
            "        -6.6266e-01, -4.1378e-01,  8.5641e-02, -3.5291e-01,  9.0650e-01,\n",
            "        -2.6775e-01,  9.3557e-01,  2.5815e-01,  5.2646e-01,  1.5271e-01,\n",
            "         1.0880e-01, -9.9207e-02, -1.6872e-02,  3.1705e-02, -5.9293e-01,\n",
            "         3.7245e-02, -9.2497e-01, -5.0368e-01, -5.5661e-01, -5.3738e-01,\n",
            "        -5.1410e-01,  2.1919e-01,  5.5329e-01,  2.3826e-01,  6.1929e-02,\n",
            "         3.1898e-01,  2.5531e-01,  2.5802e-01,  5.3434e-01,  3.1996e-01,\n",
            "         1.0069e-01, -5.5496e-01, -3.7057e-01, -3.0339e-01, -1.5533e+00,\n",
            "        -5.9777e-01,  1.9452e-01, -3.6391e-01, -1.8436e-01,  5.1853e-01,\n",
            "        -2.0573e-01,  1.6028e-01, -4.1792e-01, -5.1243e-01, -1.3041e-01,\n",
            "         6.7688e-01,  5.5768e-01, -5.3552e-01, -3.6445e-01,  3.2987e-01,\n",
            "        -2.2302e-01,  1.1296e-01,  3.6002e-01, -2.2909e-01, -1.2944e-01,\n",
            "        -6.5538e-01,  4.0744e-01, -7.1308e-01, -2.8129e-01,  2.4215e-01,\n",
            "        -4.6964e-01, -2.4974e-02,  4.9898e-01, -8.8002e-01, -5.2670e-01,\n",
            "         1.7427e-01, -4.2441e-01,  7.8662e-01, -5.3916e-01,  3.1065e-01,\n",
            "         2.8586e-01, -3.8322e-01,  9.9570e-01, -1.5867e-01,  3.1806e-01,\n",
            "         5.9172e-01,  5.0590e-01, -1.8767e-01,  1.1740e-01,  6.0312e-01,\n",
            "         5.3131e-01, -4.3883e-01,  8.9231e-01,  1.0978e-01,  2.9768e-01,\n",
            "        -1.7924e-01, -3.8839e-01, -1.0702e-01, -1.8214e-01, -5.4149e-02,\n",
            "         3.7220e-01, -8.1579e-01, -5.9574e-01, -8.2093e-01, -4.2120e-01,\n",
            "         7.8664e-01, -1.8655e-01, -1.3514e-01,  3.9490e-01,  2.2791e-01,\n",
            "        -4.9966e-01, -2.2099e-01, -9.7539e-01,  4.4585e-01, -5.9411e-01,\n",
            "        -1.5309e+00,  6.6478e-01,  3.2395e-02, -3.4876e-01, -6.6410e-01,\n",
            "         1.0171e-01, -5.4313e-02,  1.1208e-01, -1.3051e+00,  2.9222e-01,\n",
            "         2.2819e-02, -3.7351e-01,  3.6517e-01,  5.8418e-01,  5.4525e-01,\n",
            "         1.4771e-01, -6.7846e-01,  4.3451e-01,  8.1183e-01,  9.4164e-01,\n",
            "        -1.6587e-01,  2.9264e-01,  1.0431e-01, -9.7528e-01,  8.0652e-01,\n",
            "        -5.1974e-01,  6.7096e-01, -6.9639e-02,  3.8821e-01, -1.3327e-01,\n",
            "        -5.3379e-01, -2.1545e-01, -3.0924e-02,  5.2758e-01, -6.4226e-01,\n",
            "         7.4214e-01, -9.9084e-02, -2.2626e-01, -8.4757e-01, -1.4464e-01,\n",
            "        -4.9660e-01,  9.6384e-01, -3.6248e-01, -4.5590e-01,  1.7444e-01,\n",
            "        -8.6812e-01, -3.6270e-01, -8.3891e-02, -6.0712e-01, -8.8571e-01,\n",
            "        -5.2017e-01, -1.0796e-01, -1.1842e-01,  7.4888e-01, -3.6315e-01,\n",
            "        -3.6594e-01, -6.3173e-01, -2.1044e-02,  5.7322e-01, -5.0686e-01,\n",
            "         3.6577e-01, -5.8337e-01,  2.4674e-01,  5.8519e-01,  5.8704e-01,\n",
            "        -1.0922e-01, -2.2033e-01, -1.0274e-01, -2.9973e-01,  1.1981e-01,\n",
            "         1.4439e-02,  1.7719e-01, -1.6775e-02, -5.2703e-01,  6.3710e-03,\n",
            "         8.5908e-01, -9.0772e-02,  8.8936e-02, -5.2244e-01,  1.6886e-01,\n",
            "        -6.9571e-02, -3.6009e-01, -4.3534e-01,  7.3869e-01,  1.0143e-02,\n",
            "         4.2081e-02,  3.1878e-01,  4.9873e-01, -3.8344e-01, -5.5373e-01,\n",
            "        -3.9139e-01, -5.0552e-01, -8.1062e-01, -2.1466e-01,  4.6705e-01,\n",
            "         8.1063e-02, -2.4332e-02,  2.2417e-01,  1.7968e-01, -6.6669e-01,\n",
            "         8.8545e-03, -1.0320e+00, -8.4253e-01,  8.3237e-02,  2.0019e-01,\n",
            "         8.5450e-01, -4.4027e-01,  1.2960e-01,  9.1979e-01,  5.6936e-01,\n",
            "         5.6918e-01,  2.3873e-01, -9.7528e-03, -2.6828e-01, -4.3488e-01,\n",
            "        -3.1654e-01, -6.7525e-01,  8.9603e-02, -3.8492e-01,  1.8651e-01,\n",
            "         4.2965e-01,  1.6951e-01, -2.1721e-01,  4.9334e-01, -6.4151e-01,\n",
            "        -1.9921e-01,  3.0178e-01,  6.5254e-01,  1.3441e-01, -4.5364e-01,\n",
            "        -4.3964e-01,  9.3067e-01,  4.9211e-02,  7.9005e-01, -2.7947e-01,\n",
            "        -1.2861e-01, -1.1191e-01, -9.7583e-03,  9.0923e-01,  1.2004e+00,\n",
            "        -2.4724e-01, -4.1714e-01, -7.1684e-01,  1.0332e-01, -1.7323e-01,\n",
            "        -3.5575e-01, -8.4381e-01,  1.1730e+00,  5.6873e-01,  2.5399e-01,\n",
            "        -4.8529e-01, -5.6345e-01,  5.7963e-01,  5.7392e-01, -7.4179e-02,\n",
            "         1.5523e-01,  1.7210e-01, -3.5008e-01,  1.6301e-01, -8.3165e-01,\n",
            "        -9.8661e-01, -6.0110e-01, -1.2554e-01, -1.7448e+00, -3.2454e-01,\n",
            "         3.0874e-01, -3.3978e-01,  2.9525e-02, -1.3952e-01, -2.8140e-01,\n",
            "        -4.4984e-01, -3.4724e-01, -1.0021e+00,  4.8977e-01,  2.6022e-02,\n",
            "        -1.0131e+00, -4.7002e-01, -8.9715e-02, -1.0494e+00, -1.0178e-01,\n",
            "         3.8123e-02,  1.4665e-01, -2.0754e-01,  9.7096e-02,  4.6700e-01,\n",
            "        -5.9060e-02,  6.0507e-02,  5.6264e-02,  6.5830e-01, -6.8960e-01,\n",
            "        -3.5968e-01,  6.2219e-01,  2.6003e-01,  6.1821e-01,  2.0742e-01,\n",
            "         3.5915e-02, -9.6990e-01,  3.3507e-01, -8.0717e-01, -4.2299e-01,\n",
            "        -4.4527e-01, -1.5236e-01,  2.7859e-01, -1.5025e-02, -1.8272e-01,\n",
            "         6.1361e-01,  2.7269e-01,  5.0596e-01,  8.9930e-01,  4.3492e-01,\n",
            "         7.4886e-01, -3.3695e-02, -4.8587e-01,  1.9097e-02,  9.8969e-02,\n",
            "        -7.8337e-02,  2.5597e-01,  5.0859e-01,  5.3146e-01, -5.5359e-01,\n",
            "         1.4253e-01, -2.6811e-01, -4.0937e-01, -5.3445e-02,  5.1992e-01,\n",
            "         6.1826e-01, -7.5056e-01, -6.7039e-01, -6.2210e-01, -7.4448e-01,\n",
            "         2.0625e-01, -2.5697e-01,  2.4676e-01, -3.3508e-01,  7.6297e-01,\n",
            "         3.1534e-01,  5.6650e-01,  6.5713e-01, -7.9393e-02,  7.7678e-01,\n",
            "         6.8464e-01,  1.8105e-01, -5.7682e-01,  2.3148e-01,  1.8354e-01,\n",
            "         5.3264e-01,  1.0745e-01,  5.9151e-01,  1.6822e-01,  3.3482e-01,\n",
            "         2.4336e-01,  3.8000e-01,  2.3643e-01,  8.1279e-02, -1.6266e-01,\n",
            "        -2.0972e-01,  3.6127e-01,  1.7537e-01, -2.2606e-02, -3.1598e-01,\n",
            "        -3.0899e-01,  2.6050e-01,  1.6448e-01, -8.3392e-01,  2.4559e-01,\n",
            "        -1.7996e-02, -5.8636e-02,  5.7999e-01,  2.5526e-02,  1.6725e-01,\n",
            "        -2.6779e-01,  1.6890e-01,  1.3000e+00,  7.3652e-01,  2.6302e-01,\n",
            "        -2.7671e-01, -2.4274e-01, -2.1897e-01,  1.6211e-01, -3.1851e-01,\n",
            "         3.9197e-02,  1.4024e-01, -4.0862e-01,  1.9422e-01, -9.6030e-02,\n",
            "        -3.0628e-01, -3.8949e-01, -1.8944e-01, -9.0315e-01, -8.7265e-01,\n",
            "        -4.3267e-01, -7.3842e-02, -8.3184e-01, -5.2276e-01,  1.1610e-01],\n",
            "       device='cuda:0')\n",
            "tensor([1.3789e-04, 1.3163e-04, 7.5630e-05, 1.1083e-04, 9.6404e-05, 1.3945e-04,\n",
            "        2.1075e-04, 1.2519e-04, 1.0905e-04, 3.8484e-05, 9.4773e-05, 6.9633e-05,\n",
            "        1.0472e-04, 1.2593e-04, 2.7572e-04, 1.1175e-04, 7.3579e-05, 1.1755e-04,\n",
            "        1.5517e-04, 1.3978e-04, 1.8342e-04, 8.0105e-05, 1.3487e-04, 6.9928e-05,\n",
            "        1.1782e-04, 9.3183e-05, 9.3396e-05, 1.1701e-04, 1.1987e-04, 1.0378e-04,\n",
            "        1.4469e-04, 1.1704e-04, 4.3569e-04, 9.3473e-05, 3.1708e-04, 5.4145e-05,\n",
            "        1.9219e-04, 7.7250e-05, 9.0401e-05, 1.0144e-04, 1.6749e-04, 9.8975e-05,\n",
            "        1.3058e-04, 1.7978e-04, 1.0211e-04, 1.1947e-04, 7.4048e-05, 1.6887e-04,\n",
            "        1.3720e-04, 1.4427e-04, 1.0270e-04, 1.3691e-04, 1.5659e-04, 8.1339e-05,\n",
            "        3.1305e-04, 2.6110e-04, 5.6214e-05, 1.0892e-04, 1.0703e-04, 1.3314e-04,\n",
            "        1.0379e-04, 1.7033e-04, 7.4884e-05, 5.5784e-05, 1.0887e-04, 1.4640e-04,\n",
            "        1.5442e-04, 3.2324e-04, 1.7711e-04, 9.7884e-05, 1.0745e-04, 1.0405e-04,\n",
            "        4.1513e-05, 7.8393e-05, 1.4158e-04, 8.9499e-05, 8.7930e-05, 1.1980e-04,\n",
            "        1.2130e-04, 1.4422e-04, 7.2664e-05, 1.0982e-04, 1.3955e-04, 1.1245e-04,\n",
            "        2.7345e-04, 8.1630e-05, 8.7214e-05, 8.1318e-05, 1.5425e-04, 1.7110e-04,\n",
            "        1.4118e-04, 1.6119e-04, 1.8554e-04, 1.0313e-04, 8.7799e-05, 8.9123e-05,\n",
            "        2.7357e-04, 9.3545e-05, 8.5966e-05, 1.1280e-04, 1.3738e-04, 1.6158e-04,\n",
            "        1.4197e-04, 1.0698e-04, 1.8910e-04, 4.8384e-05, 5.7102e-05, 1.4921e-04,\n",
            "        2.2969e-04, 1.3530e-04, 9.2481e-05, 1.1511e-04, 1.1161e-04, 1.9278e-04,\n",
            "        1.2386e-04, 6.5334e-05, 5.4445e-05, 1.3554e-04, 5.1794e-05, 1.2266e-04,\n",
            "        1.4021e-04, 4.2960e-05, 1.1704e-04, 9.0309e-05, 8.8225e-05, 1.1028e-04,\n",
            "        6.1640e-05, 3.0059e-04, 2.0428e-04, 1.9992e-04, 1.3137e-04, 1.4122e-04,\n",
            "        2.1668e-04, 6.4144e-05, 1.6498e-04, 1.4233e-04, 2.1258e-04, 7.8264e-05,\n",
            "        5.6452e-05, 6.2896e-05, 8.9439e-05, 1.5364e-04, 5.5303e-05, 1.5147e-04,\n",
            "        3.0783e-04, 1.0719e-04, 1.6011e-04, 1.1468e-04, 1.2217e-04, 1.4054e-04,\n",
            "        8.8313e-05, 1.3968e-04, 1.3296e-03, 1.0400e-04, 2.9293e-04, 4.8759e-05,\n",
            "        1.1190e-04, 9.6276e-04, 6.0752e-05, 9.2139e-05, 5.0054e-05, 3.1633e-05,\n",
            "        3.9065e-05, 9.2205e-05, 1.0273e-04, 1.0457e-04, 9.5664e-05, 8.1959e-05,\n",
            "        1.0708e-04, 2.3750e-04, 9.4241e-05, 9.8035e-05, 5.0154e-05, 9.8013e-05,\n",
            "        4.3532e-04, 7.2511e-05, 6.6854e-05, 9.6538e-05, 9.9578e-05, 1.0188e-04,\n",
            "        4.9929e-05, 8.3605e-05, 8.7947e-05, 7.4680e-05, 7.0561e-05, 1.8282e-04,\n",
            "        2.6309e-04, 7.2201e-05, 5.8639e-05, 4.9878e-05, 7.7598e-05, 7.5406e-05,\n",
            "        1.4494e-04, 3.4571e-05, 7.4878e-05, 6.9518e-05, 3.8565e-05, 1.5405e-04,\n",
            "        7.4508e-05, 1.5597e-04, 2.1546e-04, 4.8604e-05, 4.8798e-05, 5.2085e-04,\n",
            "        7.3577e-05, 3.9061e-05, 6.5023e-05, 2.2183e-04, 6.5301e-05, 5.8235e-05,\n",
            "        7.9959e-05, 6.2658e-05, 9.5383e-05, 9.9897e-05, 2.2325e-04, 1.2132e-04,\n",
            "        1.8668e-04, 1.2824e-04, 1.1165e-04, 1.7592e-05, 1.0495e-04, 9.2807e-05,\n",
            "        1.4993e-03, 6.7850e-04, 4.6877e-04, 6.9094e-05, 1.6589e-04, 5.8123e-05,\n",
            "        1.6207e-04, 1.0674e-04, 3.5807e-04, 4.2645e-04, 3.3563e-04, 1.0953e-04,\n",
            "        9.3322e-05, 1.2578e-04, 2.0238e-04, 7.4732e-05, 1.0817e-04, 7.7360e-05,\n",
            "        2.4357e-04, 1.8592e-04, 1.0796e-04, 5.0871e-05, 8.4915e-05, 6.9439e-05,\n",
            "        7.6784e-05, 1.9430e-04, 3.0973e-03, 1.2960e-03, 5.4488e-04, 9.5718e-05,\n",
            "        9.8023e-05, 8.9066e-05, 9.8600e-05, 1.5666e-04, 2.0053e-04, 2.0865e-03,\n",
            "        8.2385e-01, 7.8747e-03, 1.4819e-03, 6.3878e-03, 8.6172e-05, 6.3148e-05,\n",
            "        9.0582e-05, 9.3243e-05, 8.4619e-05, 1.4092e-04, 7.8441e-05, 2.6809e-04,\n",
            "        1.2304e-02, 4.7932e-05, 7.9985e-05, 2.1654e-04, 1.6644e-04, 5.0570e-05,\n",
            "        1.0931e-04, 2.1297e-04, 9.4046e-05, 1.4192e-02, 1.5612e-04, 8.3996e-05,\n",
            "        5.5270e-05, 4.2677e-04, 6.2218e-05, 5.7350e-05, 4.1915e-05, 1.4657e-04,\n",
            "        1.1129e-04, 1.3403e-04, 7.4026e-05, 1.2206e-04, 8.1753e-05, 1.1236e-04,\n",
            "        4.3705e-05, 3.0996e-05, 2.2584e-04, 8.9821e-05, 5.9702e-05, 6.6557e-05,\n",
            "        6.9044e-05, 8.0268e-05, 1.1046e-04, 8.8761e-05, 9.8635e-05, 1.1450e-04,\n",
            "        1.5019e-04, 9.8471e-05, 1.4130e-04, 1.0092e-04, 9.3002e-05, 1.3743e-04,\n",
            "        1.0487e-04, 8.8093e-05, 1.5087e-04, 1.4661e-04, 7.0756e-05, 1.5266e-04,\n",
            "        1.0019e-04, 8.9877e-05, 1.7794e-04, 7.9738e-05, 6.1332e-05, 1.4459e-04,\n",
            "        3.3062e-05, 7.6429e-05, 1.3752e-04, 1.6584e-04, 2.8164e-04, 1.1311e-04,\n",
            "        9.8216e-05, 1.7547e-04, 3.1600e-04, 7.1528e-05, 1.1532e-04, 6.5687e-05,\n",
            "        4.6491e-05, 7.0886e-05, 1.1960e-04, 9.0590e-05, 1.1927e-04, 9.9092e-05,\n",
            "        9.3755e-05, 9.5763e-05, 8.8914e-05, 1.2156e-04, 2.0960e-04, 1.6328e-04,\n",
            "        9.6653e-05, 5.2254e-05, 6.9747e-05, 9.9854e-05, 7.7384e-05, 1.0219e-04,\n",
            "        9.0820e-05, 1.3552e-04, 1.0831e-04, 9.5192e-05, 2.5401e-04, 2.0472e-04,\n",
            "        3.9530e-05, 2.2288e-04, 5.6445e-05, 1.8255e-04, 5.4117e-05, 5.3253e-05,\n",
            "        2.7644e-05, 5.3367e-05, 5.3605e-05, 4.8436e-05, 1.4448e-04, 7.5397e-05,\n",
            "        4.7034e-05, 1.2617e-04, 8.8672e-05, 8.0801e-05, 4.4649e-05, 1.3134e-04,\n",
            "        7.1523e-05, 3.2104e-05, 5.4953e-05, 6.1036e-05, 5.6657e-05, 4.1905e-05,\n",
            "        3.2632e-05, 8.1589e-05, 4.4080e-05, 1.0690e-04, 7.7710e-05, 1.1003e-04,\n",
            "        8.2422e-05, 8.1855e-05, 1.4547e-04, 2.0493e-04, 9.5227e-05, 9.0778e-05,\n",
            "        1.8493e-04, 1.0937e-04, 1.9806e-04, 1.3970e-04, 1.1407e-04, 4.2206e-05,\n",
            "        2.0547e-04, 1.3798e-04, 1.2988e-04, 1.4218e-04, 8.5100e-05, 8.5006e-05,\n",
            "        1.5156e-04, 8.8605e-05, 6.4777e-05, 4.2265e-05, 6.6978e-05, 8.0290e-05,\n",
            "        7.7695e-05, 6.5445e-05, 6.3324e-05, 1.4366e-04, 7.3764e-05, 1.3444e-04,\n",
            "        9.6182e-05, 4.2716e-05, 9.3491e-05, 1.4009e-04, 7.2786e-05, 1.5183e-04,\n",
            "        1.3895e-04, 1.2550e-04, 9.4490e-05, 1.4432e-04, 9.9893e-05, 7.9236e-05,\n",
            "        1.5048e-04, 1.3881e-04, 5.6977e-05, 1.1664e-04, 1.1564e-04, 8.6143e-05,\n",
            "        6.9333e-05, 1.2793e-04, 2.0869e-04, 8.6650e-05, 1.7252e-04, 6.1625e-05,\n",
            "        1.2889e-04, 1.9012e-04, 5.3506e-05, 8.3790e-05, 1.8995e-04, 1.3785e-04,\n",
            "        9.8111e-05, 1.0324e-04, 1.4857e-04, 7.1200e-05, 9.4014e-05, 1.2759e-04,\n",
            "        1.9851e-04, 1.3801e-04, 1.0809e-04, 1.5958e-04, 1.2997e-04, 3.2863e-05,\n",
            "        2.3808e-04, 1.3638e-04, 1.6374e-04, 8.2100e-05, 6.3377e-05, 4.8518e-05,\n",
            "        1.8106e-04, 1.3192e-04, 1.6376e-04, 1.2828e-04, 1.2639e-04, 2.6574e-04,\n",
            "        5.2545e-05, 5.9563e-05, 4.4820e-04, 9.0643e-05, 2.8715e-04, 1.3936e-04,\n",
            "        1.0435e-04, 1.8083e-04, 4.3124e-05, 1.6870e-04, 1.6934e-04, 5.1399e-05,\n",
            "        1.5883e-04, 1.2264e-04, 1.6730e-04, 1.1784e-04, 1.1954e-04, 1.9027e-04,\n",
            "        6.5378e-05, 8.8203e-05, 1.0638e-04, 2.4741e-04, 1.5437e-04, 7.5246e-05,\n",
            "        1.0045e-04, 1.9100e-04, 6.2651e-05, 9.5121e-05, 1.9618e-04, 5.9070e-05,\n",
            "        5.6045e-05, 1.4401e-04, 1.2652e-04, 1.1912e-04, 1.3624e-04, 6.0251e-05,\n",
            "        1.5098e-04, 2.5691e-04, 1.2416e-04, 7.9121e-05, 1.3877e-04, 1.3033e-04,\n",
            "        4.6143e-05, 6.9147e-05, 5.7076e-05, 3.3399e-05, 9.2047e-05, 2.4921e-04,\n",
            "        3.3814e-04, 5.9347e-05, 1.0035e-04, 7.6443e-05, 6.4763e-05, 5.8407e-05,\n",
            "        8.0610e-05, 6.6059e-05, 1.6029e-04, 1.1538e-04, 1.6766e-04, 9.1680e-05,\n",
            "        1.1775e-04, 8.6828e-05, 4.3311e-05, 2.2743e-04, 1.1561e-04, 1.9959e-04,\n",
            "        1.0593e-04, 8.0767e-05, 6.8741e-05, 1.3585e-04, 1.8427e-04, 8.5826e-05,\n",
            "        1.6034e-04, 1.0730e-04, 5.9759e-05, 1.5501e-04, 1.0081e-04, 1.5739e-04,\n",
            "        1.0534e-04, 3.9192e-05, 1.0880e-04, 7.8521e-05, 1.2034e-04, 1.6677e-04,\n",
            "        1.2310e-04, 1.7814e-04, 9.2497e-05, 4.5738e-05, 2.1370e-04, 1.8028e-04,\n",
            "        1.4479e-04, 9.5273e-05, 1.2210e-04, 1.5333e-04, 8.7921e-05, 7.2575e-05,\n",
            "        1.0344e-04, 5.8679e-05, 1.1788e-04, 2.5330e-04, 1.0116e-04, 8.4435e-05,\n",
            "        1.6069e-04, 1.0416e-04, 1.7503e-04, 9.0930e-05, 1.2575e-04, 1.5500e-04,\n",
            "        1.0080e-04, 1.1768e-04, 2.2357e-04, 1.8615e-04, 5.6471e-05, 1.5928e-04,\n",
            "        7.1527e-05, 1.0970e-04, 1.9024e-04, 1.2266e-04, 2.2716e-04, 1.7793e-04,\n",
            "        1.8949e-04, 6.3696e-05, 2.9685e-04, 1.3345e-04, 6.4111e-05, 8.1671e-05,\n",
            "        6.0754e-05, 7.7922e-05, 1.2840e-04, 8.2812e-05, 2.9178e-04, 9.0174e-05,\n",
            "        3.0038e-04, 1.5257e-04, 1.9953e-04, 1.3730e-04, 1.3140e-04, 1.0673e-04,\n",
            "        1.1589e-04, 1.2166e-04, 6.5141e-05, 1.2233e-04, 4.6736e-05, 7.1222e-05,\n",
            "        6.7550e-05, 6.8862e-05, 7.0484e-05, 1.4674e-04, 2.0495e-04, 1.4957e-04,\n",
            "        1.2539e-04, 1.6214e-04, 1.5214e-04, 1.5255e-04, 2.0110e-04, 1.6230e-04,\n",
            "        1.3034e-04, 6.7662e-05, 8.1362e-05, 8.7017e-05, 2.4933e-05, 6.4827e-05,\n",
            "        1.4317e-04, 8.1907e-05, 9.8015e-05, 1.9795e-04, 9.5943e-05, 1.3835e-04,\n",
            "        7.7600e-05, 7.0602e-05, 1.0345e-04, 2.3191e-04, 2.0585e-04, 6.8991e-05,\n",
            "        8.1862e-05, 1.6392e-04, 9.4299e-05, 1.3195e-04, 1.6893e-04, 9.3728e-05,\n",
            "        1.0355e-04, 6.1197e-05, 1.7714e-04, 5.7766e-05, 8.8961e-05, 1.5015e-04,\n",
            "        7.3689e-05, 1.1495e-04, 1.9412e-04, 4.8885e-05, 6.9602e-05, 1.4030e-04,\n",
            "        7.7098e-05, 2.5881e-04, 6.8740e-05, 1.6080e-04, 1.5686e-04, 8.0340e-05,\n",
            "        3.1900e-04, 1.0057e-04, 1.6199e-04, 2.1298e-04, 1.9547e-04, 9.7691e-05,\n",
            "        1.3254e-04, 2.1542e-04, 2.0050e-04, 7.5994e-05, 2.8767e-04, 1.3153e-04,\n",
            "        1.5872e-04, 9.8518e-05, 7.9926e-05, 1.0590e-04, 9.8233e-05, 1.1165e-04,\n",
            "        1.7100e-04, 5.2128e-05, 6.4958e-05, 5.1861e-05, 7.7346e-05, 2.5882e-04,\n",
            "        9.7802e-05, 1.0296e-04, 1.7493e-04, 1.4803e-04, 7.1509e-05, 9.4490e-05,\n",
            "        4.4438e-05, 1.8407e-04, 6.5064e-05, 2.5496e-05, 2.2912e-04, 1.2174e-04,\n",
            "        8.3156e-05, 6.0666e-05, 1.3048e-04, 1.1163e-04, 1.3184e-04, 3.1956e-05,\n",
            "        1.5786e-04, 1.2058e-04, 8.1124e-05, 1.6981e-04, 2.1138e-04, 2.0331e-04,\n",
            "        1.3662e-04, 5.9801e-05, 1.8200e-04, 2.6542e-04, 3.0221e-04, 9.9845e-05,\n",
            "        1.5793e-04, 1.3082e-04, 4.4443e-05, 2.6401e-04, 7.0088e-05, 2.3055e-04,\n",
            "        1.0993e-04, 1.7376e-04, 1.0315e-04, 6.9110e-05, 9.5015e-05, 1.1427e-04,\n",
            "        1.9975e-04, 6.2006e-05, 2.4755e-04, 1.0674e-04, 9.3994e-05, 5.0497e-05,\n",
            "        1.0199e-04, 7.1729e-05, 3.0899e-04, 8.2024e-05, 7.4708e-05, 1.4032e-04,\n",
            "        4.9470e-05, 8.2005e-05, 1.0837e-04, 6.4223e-05, 4.8608e-05, 7.0058e-05,\n",
            "        1.0580e-04, 1.0470e-04, 2.4923e-04, 8.1969e-05, 8.1740e-05, 6.2662e-05,\n",
            "        1.1540e-04, 2.0908e-04, 7.0996e-05, 1.6991e-04, 6.5767e-05, 1.5084e-04,\n",
            "        2.1160e-04, 2.1199e-04, 1.0566e-04, 9.4552e-05, 1.0635e-04, 8.7335e-05,\n",
            "        1.3286e-04, 1.1957e-04, 1.4071e-04, 1.1590e-04, 6.9578e-05, 1.1861e-04,\n",
            "        2.7826e-04, 1.0763e-04, 1.2882e-04, 6.9899e-05, 1.3954e-04, 1.0994e-04,\n",
            "        8.2220e-05, 7.6260e-05, 2.4670e-04, 1.1906e-04, 1.2292e-04, 1.6211e-04,\n",
            "        1.9407e-04, 8.0322e-05, 6.7745e-05, 7.9686e-05, 7.1091e-05, 5.2398e-05,\n",
            "        9.5090e-05, 1.8802e-04, 1.2781e-04, 1.1503e-04, 1.4747e-04, 1.4106e-04,\n",
            "        6.0509e-05, 1.1891e-04, 4.1991e-05, 5.0752e-05, 1.2809e-04, 1.4398e-04,\n",
            "        2.7699e-04, 7.5884e-05, 1.3417e-04, 2.9568e-04, 2.0827e-04, 2.0823e-04,\n",
            "        1.4964e-04, 1.1671e-04, 9.0126e-05, 7.6295e-05, 8.5880e-05, 5.9993e-05,\n",
            "        1.2891e-04, 8.0204e-05, 1.4202e-04, 1.8111e-04, 1.3963e-04, 9.4848e-05,\n",
            "        1.9303e-04, 6.2052e-05, 9.6571e-05, 1.5938e-04, 2.2634e-04, 1.3481e-04,\n",
            "        7.4877e-05, 7.5932e-05, 2.9892e-04, 1.2380e-04, 2.5970e-04, 8.9123e-05,\n",
            "        1.0363e-04, 1.0538e-04, 1.1671e-04, 2.9257e-04, 3.9147e-04, 9.2042e-05,\n",
            "        7.7660e-05, 5.7550e-05, 1.3069e-04, 9.9113e-05, 8.2578e-05, 5.0687e-05,\n",
            "        3.8089e-04, 2.0814e-04, 1.5194e-04, 7.2544e-05, 6.7090e-05, 2.1042e-04,\n",
            "        2.0922e-04, 1.0943e-04, 1.3765e-04, 1.3999e-04, 8.3047e-05, 1.3873e-04,\n",
            "        5.1307e-05, 4.3942e-05, 6.4611e-05, 1.0395e-04, 2.0588e-05, 8.5195e-05,\n",
            "        1.6049e-04, 8.3906e-05, 1.2139e-04, 1.0251e-04, 8.8951e-05, 7.5162e-05,\n",
            "        8.3283e-05, 4.3266e-05, 1.9234e-04, 1.2097e-04, 4.2794e-05, 7.3660e-05,\n",
            "        1.0775e-04, 4.1266e-05, 1.0645e-04, 1.2244e-04, 1.3647e-04, 9.5770e-05,\n",
            "        1.2988e-04, 1.8801e-04, 1.1110e-04, 1.2521e-04, 1.2468e-04, 2.2765e-04,\n",
            "        5.9139e-05, 8.2254e-05, 2.1957e-04, 1.5286e-04, 2.1870e-04, 1.4502e-04,\n",
            "        1.2217e-04, 4.4683e-05, 1.6477e-04, 5.2579e-05, 7.7208e-05, 7.5506e-05,\n",
            "        1.0120e-04, 1.5572e-04, 1.1610e-04, 9.8177e-05, 2.1770e-04, 1.5481e-04,\n",
            "        1.9548e-04, 2.8968e-04, 1.8207e-04, 2.4922e-04, 1.1395e-04, 7.2502e-05,\n",
            "        1.2013e-04, 1.3012e-04, 1.0898e-04, 1.5224e-04, 1.9599e-04, 2.0053e-04,\n",
            "        6.7755e-05, 1.3591e-04, 9.0142e-05, 7.8266e-05, 1.1173e-04, 1.9823e-04,\n",
            "        2.1871e-04, 5.5641e-05, 6.0286e-05, 6.3268e-05, 5.5981e-05, 1.4486e-04,\n",
            "        9.1151e-05, 1.5084e-04, 8.4302e-05, 2.5276e-04, 1.6155e-04, 2.0768e-04,\n",
            "        2.2738e-04, 1.0886e-04, 2.5628e-04, 2.3372e-04, 1.4125e-04, 6.6199e-05,\n",
            "        1.4856e-04, 1.4160e-04, 2.0076e-04, 1.3123e-04, 2.1294e-04, 1.3945e-04,\n",
            "        1.6473e-04, 1.5033e-04, 1.7234e-04, 1.4929e-04, 1.2784e-04, 1.0017e-04,\n",
            "        9.5561e-05, 1.6914e-04, 1.4045e-04, 1.1522e-04, 8.5927e-05, 8.6531e-05,\n",
            "        1.5293e-04, 1.3893e-04, 5.1191e-05, 1.5067e-04, 1.1576e-04, 1.1115e-04,\n",
            "        2.1050e-04, 1.2091e-04, 1.3931e-04, 9.0170e-05, 1.3954e-04, 4.3246e-04,\n",
            "        2.4617e-04, 1.5332e-04, 8.9369e-05, 9.2457e-05, 9.4682e-05, 1.3860e-04,\n",
            "        8.5710e-05, 1.2257e-04, 1.3560e-04, 7.8325e-05, 1.4312e-04, 1.0707e-04,\n",
            "        8.6765e-05, 7.9838e-05, 9.7519e-05, 4.7767e-05, 4.9246e-05, 7.6463e-05,\n",
            "        1.0947e-04, 5.1298e-05, 6.9876e-05, 1.3237e-04], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
        "model.eval()\n",
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
        "print(output[0])\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNV3MXGUZOrTXYIMUGxxCsi",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
